{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "933af709",
   "metadata": {
    "id": "933af709"
   },
   "source": [
    "# COMP 345: Assignment 1 (104 points)\n",
    "\n",
    "This assignment will help you practice regular expressions for text pattern matching, working with dictionaries to analyze word frequencies and compute statistics, and implementing a Naive Bayes text classifier from scratch including tokenization, probability computation, and model evaluation.\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "For each exercise in this notebook:\n",
    "- Read the problem description carefully\n",
    "- Write your solution **only** between the designated code markers:\n",
    "  ```python\n",
    "  ### WRITE YOUR CODE BELOW\n",
    "  # Your code here\n",
    "  ### END CODE HERE\n",
    "  ```\n",
    "- Do **not** modify any code outside these markers\n",
    "- Do **not** change function signatures (function name, parameters, return type)\n",
    "- Do **not** change test cells or example outputs\n",
    "- Run each cell to verify your solution works correctly\n",
    "\n",
    "## How to Submit the Assignment?\n",
    "\n",
    "1. **Create a copy of the assignment**: Before starting, create a copy of this notebook in your Google Drive by clicking `File > Save a copy in Drive`. This ensures your progress is saved as you work.\n",
    "\n",
    "2. **Complete all exercises**: Work through each exercise in your copied notebook, writing your solutions between the designated code markers.\n",
    "\n",
    "3. **Download the notebook**: Once you have completed all exercises, download the notebook as an `.ipynb` file by clicking `File > Download > Download .ipynb` as shown below:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://drive.google.com/thumbnail?id=1SZc-bK8PzBmMsgI5iUY4g9AvKk128qO4&sz=w800\" alt=\"Download notebook from Colab\" width=\"800\">\n",
    "</p>\n",
    "\n",
    "4. **Rename the file**: Rename the downloaded file to `<student_id>_A1.ipynb` (e.g., `9284827_A1.ipynb`).\n",
    "\n",
    "5. **Submit on Gradescope**: Upload the renamed notebook file to Gradescope.\n",
    "https://www.gradescope.ca/courses/35049/assignments/178742\n",
    "\n",
    "## Questions?\n",
    "\n",
    "If you have any questions about the assignment, please reach out to the TA:\n",
    "- Slack: `#assignment-1`\n",
    "- Email: `jay.gala@mila.quebec` (**Note:** Please include `[COMP 345]` in the subject)\n",
    "- Office Hours: Tuesdays and Thursdays, 2:45 pm – 3:45 pm in McConnell Engineering Building Room 110 (Jan 19, 22, 27, 29)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5d215",
   "metadata": {
    "id": "dbf5d215"
   },
   "source": [
    "## AI Usage Disclosure\n",
    "\n",
    "If you used any AI tools while completing this assignment, you must disclose this below. Please specify:\n",
    "\n",
    "1. **Which AI tool(s) did you use?**\n",
    "2. **For which exercises or tasks did you use them?**\n",
    "3. **How did you use them?**\n",
    "\n",
    "**Note:** Please refer to the course website for details on the [Generative AI Policy](https://mcgill-nlp.github.io/teaching/comp345-ling345-W26/#generative-ai-policy).\n",
    "\n",
    "---\n",
    "\n",
    "**Your disclosure (edit this cell):**\n",
    "\n",
    "- AI tool(s) used: <Claude.ai>\n",
    "- Exercises: <for each exercise, I used it to check my work>\n",
    "- How used: <I provided the questions and my code, and asked if it was correct or it needed improvements. I improved my answers based on the feedback when necessary.>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611132b1",
   "metadata": {
    "id": "611132b1"
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7515279d",
   "metadata": {
    "id": "7515279d"
   },
   "source": [
    "## 1. Regular Expressions (35 points)\n",
    "\n",
    "This section tests your understanding of regular expressions (regex) for pattern matching in text. You'll work with Python's `re` module to find patterns, extract information, validate formats, and process text data using various regex features including character classes, quantifiers, anchors, and groups.\n",
    "\n",
    "**Note on `re` module methods:** While the lecture primarily covered `re.findall()`, this assignment will also use other methods from the `re` module:\n",
    "\n",
    "- **`re.findall(pattern, string)`**: Returns a list of all matches in the string. Use this when you need to extract all occurrences of a pattern.\n",
    "- **`re.search(pattern, string)`**: Searches for the first occurrence of the pattern anywhere in the string. Returns a match object if found, `None` otherwise. Use this when you need to check if a pattern exists.\n",
    "- **`re.match(pattern, string)`**: Checks if the pattern matches at the beginning of the string. Returns a match object if it matches, `None` otherwise. Use this for validation when the entire string should match a pattern (often with anchors `^` and `$`).\n",
    "\n",
    "For more details and examples, refer to the [Python `re` module documentation](https://docs.python.org/3/library/re.html) and [Python Regular Expressions Google blogpost](https://developers.google.com/edu/python/regular-expressions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53be1c2b",
   "metadata": {
    "id": "53be1c2b"
   },
   "source": [
    "### 1.1: Email Validator (9 points)\n",
    "\n",
    "You're building a simple form validator that needs to check if user input contains valid email addresses. While real email validation is complex, you'll implement a basic version using regex patterns to match common email formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e9765",
   "metadata": {
    "id": "af3e9765"
   },
   "source": [
    "#### 1.1.1: Extract All Emails (3 points)\n",
    "\n",
    "This function extracts all email addresses from a given text. An email should match the pattern:\n",
    "- Local part (before @): one or more characters including letters (a-z, A-Z), digits (0-9), and punctuation characters (. _ % + -)\n",
    "- @ symbol\n",
    "- Domain name (after @): one or more characters including letters, digits, and punctuation characters (. -)\n",
    "- A dot (.) followed by 2 or more letters for the domain extension\n",
    "\n",
    "**Note:** The first character should be always start as alphanumeric. So the string that starts with `%abc123@gmail.com` will be invalid email.\n",
    "\n",
    "Arguments:\n",
    "- `text (str)`: The input text to search for email addresses.\n",
    "\n",
    "Returns:\n",
    "- `emails (List[str])`: A list of all email addresses found in the text (in order of appearance).\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> text = \"Contact us at support@example.com or sales@company.org for help.\"\n",
    ">>> extract_emails(text)\n",
    "['support@example.com', 'sales@company.org']\n",
    ">>> text = \"No emails here!\"\n",
    ">>> extract_emails(text)\n",
    "[]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1816b1",
   "metadata": {
    "id": "3f1816b1"
   },
   "outputs": [],
   "source": [
    "def extract_emails(text: str) -> List[str]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    # first char: alphanumeric, rest: can have letters, digits, . _ % + -\n",
    "    # @ + (domain: letters, digits, . - ) + \".\"  +  (2+ letters) \n",
    "    \n",
    "    pattern = r'[a-zA-Z0-9][a-zA-Z0-9._%+-]*@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "    emails = re.findall(pattern, text)\n",
    "    \n",
    "    ### END CODE HERE\n",
    "    return emails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac971de",
   "metadata": {
    "id": "eac971de"
   },
   "source": [
    "#### 1.1.2: Validate Email Format (4 points)\n",
    "\n",
    "This function checks if a single string is a valid email address. The entire string must match the email pattern exactly with no extra characters before or after. A valid email should have:\n",
    "- Local part (before @): one or more characters including letters (a-z, A-Z), digits (0-9), and punctuation characters (. _ % + -)\n",
    "- @ symbol\n",
    "- Domain name (after @): one or more characters including letters, digits, and punctuation characters (. -)\n",
    "- A dot (.) followed by 2 or more letters for the domain extension\n",
    "\n",
    "**Note:** The first character should be always start as alphanumeric. So the string that starts with `%abc123@gmail.com` will be invalid email.\n",
    "\n",
    "Arguments:\n",
    "- `email (str)`: The string to validate.\n",
    "\n",
    "Returns:\n",
    "- `is_valid (bool)`: True if the string is a valid email format, False otherwise.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> validate_email(\"user@example.com\")\n",
    "True\n",
    ">>> validate_email(\"invalid.email@\")\n",
    "False\n",
    ">>> validate_email(\"also@invalid\")\n",
    "False\n",
    ">>> validate_email(\"This is not an email\")\n",
    "False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0608ccf3",
   "metadata": {
    "id": "0608ccf3"
   },
   "outputs": [],
   "source": [
    "def validate_email(email: str) -> bool:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    # same as extract_emails + anchors to match entire string\n",
    "    \n",
    "    pattern = r'^[a-zA-Z0-9][a-zA-Z0-9._%+-]*@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    is_valid = re.match(pattern, email) is not None\n",
    "    \n",
    "    ### END CODE HERE\n",
    "    return is_valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53179d7a",
   "metadata": {
    "id": "53179d7a"
   },
   "source": [
    "#### 1.1.3: Extract Email Domains (2 points)\n",
    "\n",
    "This function extracts unique domain names (the part after @) from all email addresses found in a text, returning them sorted alphabetically.\n",
    "\n",
    "Arguments:\n",
    "- `text (str)`: The input text containing email addresses.\n",
    "\n",
    "Returns:\n",
    "- `domains (List[str])`: A sorted list of unique domain names (e.g., \"example.com\").\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> text = \"Email us at support@example.com or info@example.com and admin@test.org\"\n",
    ">>> extract_domains(text)\n",
    "['example.com', 'test.org']\n",
    ">>> text = \"No emails here\"\n",
    ">>> extract_domains(text)\n",
    "[]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcddc072",
   "metadata": {
    "id": "fcddc072"
   },
   "outputs": [],
   "source": [
    "def extract_domains(text: str) -> List[str]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    \n",
    "    emails = extract_emails(text)\n",
    "    domains = []  # after @\n",
    "    for email in emails:\n",
    "        domain = email.split('@')[1]\n",
    "        domains.append(domain)\n",
    "    domains = sorted(list(set(domains)))\n",
    "    \n",
    "    ### END CODE HERE\n",
    "    return domains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd372ea",
   "metadata": {
    "id": "9dd372ea"
   },
   "source": [
    "### 1.2: Phone Number Extractor (5 points)\n",
    "\n",
    "You're working on a contact management system that needs to extract and standardize phone numbers from various text sources. Phone numbers can appear in different formats, and you need to identify and process them consistently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2514fc",
   "metadata": {
    "id": "cc2514fc"
   },
   "source": [
    "#### 1.2.1: Find Simple Phone Numbers (2 points)\n",
    "\n",
    "This function finds all phone numbers in the format XXX-XXXX (3 digits, hyphen, 4 digits) in a text.\n",
    "\n",
    "Arguments:\n",
    "- `text (str)`: The input text to search.\n",
    "\n",
    "Returns:\n",
    "- `phone_numbers (List[str])`: A list of all phone numbers found in XXX-XXXX format.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> text = \"Call me at 555-1234 or 555-5678 for more info.\"\n",
    ">>> find_simple_phones(text)\n",
    "['555-1234', '555-5678']\n",
    ">>> text = \"My number is 12345678\"\n",
    ">>> find_simple_phones(text)\n",
    "[]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de50b5ab",
   "metadata": {
    "id": "de50b5ab"
   },
   "outputs": [],
   "source": [
    "def find_simple_phones(text: str) -> List[str]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    # XXX-XXXX: 3 digits, hyphen, 4 digits\n",
    "    pattern = r'\\d{3}-\\d{4}'\n",
    "    phone_numbers = re.findall(pattern, text)\n",
    "    ### END CODE HERE\n",
    "    return phone_numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc894b",
   "metadata": {
    "id": "e5dc894b"
   },
   "source": [
    "#### 1.2.2: Find US Phone Numbers (3 points)\n",
    "\n",
    "This function finds phone numbers in the format (XXX) XXX-XXXX, where X is a digit. This represents the common US phone number format with area code in parentheses.\n",
    "\n",
    "Arguments:\n",
    "- `text (str)`: The input text to search.\n",
    "\n",
    "Returns:\n",
    "- `phone_numbers (List[str])`: A list of all US-format phone numbers found.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> text = \"Contact: (555) 123-4567 or (800) 555-0199\"\n",
    ">>> find_us_phones(text)\n",
    "['(555) 123-4567', '(800) 555-0199']\n",
    ">>> text = \"Call 555-1234\"\n",
    ">>> find_us_phones(text)\n",
    "[]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7d04e9",
   "metadata": {
    "id": "6f7d04e9"
   },
   "outputs": [],
   "source": [
    "def find_us_phones(text: str) -> List[str]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    # (XXX) XXX-XXXX - escape parentheses\n",
    "    pattern = r'\\(\\d{3}\\) \\d{3}-\\d{4}'\n",
    "    phone_numbers = re.findall(pattern, text)\n",
    "    ### END CODE HERE\n",
    "    return phone_numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305ef0b7",
   "metadata": {
    "id": "305ef0b7"
   },
   "source": [
    "### 1.3: Date and Time Parser (6 points)\n",
    "\n",
    "You're building a log analyzer that needs to extract dates and timestamps from various log files. Different systems use different date formats, so you need flexible regex patterns to handle them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcd079f",
   "metadata": {
    "id": "cdcd079f"
   },
   "source": [
    "#### 1.3.1: Extract Dates in MM/DD/YYYY Format (3 points)\n",
    "\n",
    "This function finds all dates in the format MM/DD/YYYY where MM and DD can be 1 or 2 digits, and YYYY is exactly 4 digits.\n",
    "\n",
    "Arguments:\n",
    "- `text (str)`: The input text containing dates.\n",
    "\n",
    "Returns:\n",
    "- `dates (List[str])`: A list of all dates found in MM/DD/YYYY format.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> text = \"Important dates: 12/25/2023 and 1/1/2024\"\n",
    ">>> extract_dates_mdy(text)\n",
    "['12/25/2023', '1/1/2024']\n",
    ">>> text = \"No dates here\"\n",
    ">>> extract_dates_mdy(text)\n",
    "[]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5837e2a7",
   "metadata": {
    "id": "5837e2a7"
   },
   "outputs": [],
   "source": [
    "def extract_dates_mdy(text: str) -> List[str]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    # MM/DD/YYYY: MM and DD --> 1-2 digits, YYYY --> 4 digits\n",
    "    pattern = r'\\d{1,2}/\\d{1,2}/\\d{4}'\n",
    "    dates = re.findall(pattern, text)\n",
    "    ### END CODE HERE\n",
    "    return dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587ecd05",
   "metadata": {
    "id": "587ecd05"
   },
   "source": [
    "#### 1.3.2: Extract Timestamps (3 points)\n",
    "\n",
    "This function extracts timestamps in the format HH:MM:SS where each component is exactly 2 digits.\n",
    "\n",
    "Arguments:\n",
    "- `text (str)`: The input text containing timestamps.\n",
    "\n",
    "Returns:\n",
    "- `timestamps (List[str])`: A list of all timestamps found in HH:MM:SS format.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> text = \"Events at 14:30:00 and 09:15:45\"\n",
    ">>> extract_timestamps(text)\n",
    "['14:30:00', '09:15:45']\n",
    ">>> text = \"Time is 9:5:3\"\n",
    ">>> extract_timestamps(text)\n",
    "[]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cb89c7",
   "metadata": {
    "id": "a1cb89c7"
   },
   "outputs": [],
   "source": [
    "def extract_timestamps(text: str) -> List[str]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    # HH:MM:SS --> each component: 2 digits\n",
    "    pattern = r'\\d{2}:\\d{2}:\\d{2}'\n",
    "    timestamps = re.findall(pattern, text)\n",
    "    ### END CODE HERE\n",
    "    return timestamps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3698e409",
   "metadata": {
    "id": "3698e409"
   },
   "source": [
    "### 1.4: Social Media Text Processor (8 points)\n",
    "\n",
    "You're developing a social media analytics tool that needs to extract and analyze various elements from posts and tweets, including hashtags, mentions, and URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7926dd9c",
   "metadata": {
    "id": "7926dd9c"
   },
   "source": [
    "#### 1.4.1: Extract Hashtags (2 points)\n",
    "\n",
    "This function extracts all hashtags from social media text. A valid hashtag must:\n",
    "- Start with a # symbol that is NOT immediately preceded by a word character (letter, digit, or underscore)\n",
    "- Be followed by one or more word characters (letters, digits, or underscores)\n",
    "\n",
    "This means:\n",
    "- `#Python` is a valid hashtag\n",
    "- `#AI_ML` is a valid hashtag (underscores are allowed)\n",
    "- `#123` is a valid hashtag (digits are allowed)\n",
    "- `test#nothashtag` is NOT a valid hashtag (# is preceded by a word character)\n",
    "- `#` alone is NOT a valid hashtag (no word characters after #)\n",
    "\n",
    "Arguments:\n",
    "- `text (str)`: The social media post text.\n",
    "\n",
    "Returns:\n",
    "- `hashtags (List[str])`: A list of all hashtags found (including the # symbol).\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> text = \"Check out #Python and #DataScience for #AI_ML projects!\"\n",
    ">>> extract_hashtags(text)\n",
    "['#Python', '#DataScience', '#AI_ML']\n",
    ">>> text = \"No hashtags here\"\n",
    ">>> extract_hashtags(text)\n",
    "[]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95391aff",
   "metadata": {
    "id": "95391aff"
   },
   "outputs": [],
   "source": [
    "def extract_hashtags(text: str) -> List[str]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    # '#' is not preceded by \\w\n",
    "    pattern = r'(?<!\\w)#\\w+'\n",
    "    hashtags = re.findall(pattern, text)\n",
    "    ### END CODE HERE\n",
    "    return hashtags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cf64ae",
   "metadata": {
    "id": "03cf64ae"
   },
   "source": [
    "#### 1.4.2: Extract Mentions (2 points)\n",
    "\n",
    "This function extracts all user mentions from social media text. A valid mention must:\n",
    "- Start with an @ symbol that is NOT immediately preceded by a word character (letter, digit, or underscore)\n",
    "- Be followed by one or more word characters (letters, digits, or underscores)\n",
    "\n",
    "This means:\n",
    "- `@user123` is a valid mention\n",
    "- `@admin` is a valid mention\n",
    "- `@user_name` is a valid mention (underscores are allowed)\n",
    "- `info@example.com` is NOT a valid mention (@ is preceded by a word character, so it's recognized as an email)\n",
    "- `@` alone is NOT a valid mention (no word characters after @)\n",
    "\n",
    "Arguments:\n",
    "- `text (str)`: The social media post text.\n",
    "\n",
    "Returns:\n",
    "- `mentions (List[str])`: A list of all mentions found (including the @ symbol).\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> text = \"Thanks @user123 and @admin for the help! You can also reach out to info@example.com\"\n",
    ">>> extract_mentions(text)\n",
    "['@user123', '@admin']\n",
    ">>> text = \"No mentions in this post\"\n",
    ">>> extract_mentions(text)\n",
    "[]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7ce89",
   "metadata": {
    "id": "64a7ce89"
   },
   "outputs": [],
   "source": [
    "def extract_mentions(text: str) -> List[str]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    # @ not preceded by word char  +    1+ word characters\n",
    "    pattern = r'(?<!\\w)@\\w+'\n",
    "    mentions = re.findall(pattern, text)\n",
    "    ### END CODE HERE\n",
    "    return mentions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f575a73b",
   "metadata": {
    "id": "f575a73b"
   },
   "source": [
    "#### 1.4.3: Extract URLs (4 points)\n",
    "\n",
    "This function extracts all URLs from text. A URL starts with http:// or https:// followed by one or more non-whitespace characters.\n",
    "\n",
    "Arguments:\n",
    "- `text (str)`: The text containing URLs.\n",
    "\n",
    "Returns:\n",
    "- `urls (List[str])`: A list of all URLs found.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> text = \"Visit https://example.com and http://test.org for more info\"\n",
    ">>> extract_urls(text)\n",
    "['https://example.com', 'http://test.org']\n",
    ">>> text = \"No links here\"\n",
    ">>> extract_urls(text)\n",
    "[]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a6779c",
   "metadata": {
    "id": "46a6779c"
   },
   "outputs": [],
   "source": [
    "def extract_urls(text: str) -> List[str]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    # http:// or https://      +      1+ non-whitespace chars\n",
    "    pattern = r'https?://\\S+'\n",
    "    urls = re.findall(pattern, text)\n",
    "    ### END CODE HERE\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bd2834",
   "metadata": {
    "id": "56bd2834"
   },
   "source": [
    "### 1.5: Advanced Pattern Matching (7 points)\n",
    "\n",
    "These exercises involve more complex regex patterns using groups, alternation, and advanced pattern matching techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da9716",
   "metadata": {
    "id": "07da9716"
   },
   "source": [
    "#### 1.5.1: Extract Price Information (3 points)\n",
    "\n",
    "This function extracts monetary amounts from text. Prices can be in various formats: \\$X, \\$X.XX. The function should handle optional cents (two digits after decimal point) and return all unique prices found, sorted in ascending order by their numeric value.\n",
    "\n",
    "Arguments:\n",
    "- `text (str)`: The input text containing prices.\n",
    "\n",
    "Returns:\n",
    "- `prices (List[str])`: A sorted list of unique price strings found (keeping their original format).\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> text = \"Items cost $19.99, $5, and $125.50 respectively\"\n",
    ">>> extract_prices(text)\n",
    "['$5', '$19.99', '$125.50']\n",
    ">>> text = \"The book is $29 and the pen is $2.50\"\n",
    ">>> extract_prices(text)\n",
    "['$2.50', '$29']\n",
    ">>> text = \"No prices here\"\n",
    ">>> extract_prices(text)\n",
    "[]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d3b123",
   "metadata": {
    "id": "71d3b123"
   },
   "outputs": [],
   "source": [
    "def extract_prices(text: str) -> List[str]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    # $ + digits + (optional) .XX\n",
    "    pattern = r'\\$\\d+(?:\\.\\d{2})?'\n",
    "    prices = re.findall(pattern, text)\n",
    "    prices = list(set(prices))\n",
    "    prices.sort(key=lambda x: float(x.replace('$', '')))\n",
    "    ### END CODE HERE\n",
    "    return prices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d915613",
   "metadata": {
    "id": "2d915613"
   },
   "source": [
    "#### 1.5.2: Validate Password Strength (4 points)\n",
    "\n",
    "This function validates whether a password meets security requirements using regex. A strong password must:\n",
    "1. Be at least 8 characters long\n",
    "2. Contain at least one uppercase letter\n",
    "3. Contain at least one lowercase letter\n",
    "4. Contain at least one digit\n",
    "5. Contain at least one special character from: !@#$%^&*()\n",
    "\n",
    "Arguments:\n",
    "- `password (str)`: The password string to validate.\n",
    "\n",
    "Returns:\n",
    "- `is_valid (bool)`: True if the password meets all requirements, False otherwise.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> validate_password(\"Abc123!@\")\n",
    "True\n",
    ">>> validate_password(\"weak\")\n",
    "False\n",
    ">>> validate_password(\"NoDigits!\")\n",
    "False\n",
    ">>> validate_password(\"nouppercas3!\")\n",
    "False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec2e67a",
   "metadata": {
    "id": "eec2e67a"
   },
   "outputs": [],
   "source": [
    "def validate_password(password: str) -> bool:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    if len(password) < 8:\n",
    "        return False\n",
    "    if not re.search(r'[A-Z]', password):  # 1+ uppercase\n",
    "        return False\n",
    "    if not re.search(r'[a-z]', password):  # 1+ lowercase\n",
    "        return False\n",
    "    if not re.search(r'\\d', password):  # 1+ digit\n",
    "        return False\n",
    "    if not re.search(r'[!@#$%^&*()]', password):  # 1+ special char\n",
    "        return False\n",
    "    is_valid = True\n",
    "    ### END CODE HERE\n",
    "    return is_valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a894d0e",
   "metadata": {
    "id": "9a894d0e"
   },
   "source": [
    "## 2. Tokenization (16 points)\n",
    "\n",
    "This section tests your understanding of tokenization. You'll build a simple Byte-Pair Encoding (BPE) tokenizer from scratch, learning how modern language models break text into tokens. BPE is used by models like GPT to handle rare words and reduce vocabulary size by learning common character sequences.\n",
    "\n",
    "### Background\n",
    "\n",
    "BPE works by iteratively merging the most frequent pairs of characters (or character sequences) in a corpus. Starting with individual characters, it builds up a vocabulary of subwords that efficiently represent the text.\n",
    "\n",
    "In this implementation, we use a special start-of-word marker `_` (underscore) to distinguish word boundaries. Following the convention used by GPT-2 (which uses `Ġ`) and SentencePiece (which uses `▁`), the marker is **attached to the first character** of each word during the initial tokenization. For example, the text \"hello world\" would be tokenized as `['_h', 'e', 'l', 'l', 'o', ' ', '_w', 'o', 'r', 'l', 'd']`. This way, tokens like `_t` (start of \"the\", \"to\", etc.) can be learned directly through BPE merges.\n",
    "\n",
    "You can read more about BPE tokenization on [Hugging Face](https://huggingface.co/learn/llm-course/en/chapter6/5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2db0502",
   "metadata": {
    "id": "e2db0502"
   },
   "outputs": [],
   "source": [
    "### DO NOT MODIFY THIS CELL ###\n",
    "CORPUS = \"\"\"To be or not to be that is the question.\n",
    "Whether tis nobler in the mind to suffer the slings and arrows of outrageous fortune.\n",
    "Or to take arms against a sea of troubles and by opposing end them.\n",
    "To die to sleep no more and by a sleep to say we end the heartache.\n",
    "And the thousand natural shocks that flesh is heir to.\"\"\"\n",
    "\n",
    "print(\"Training corpus:\")\n",
    "print(CORPUS)\n",
    "print(f\"\\nCorpus size: {len(CORPUS)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf1fc5a",
   "metadata": {
    "id": "acf1fc5a"
   },
   "source": [
    "### 2.1: Tokenize Text into Characters with Word Markers (2 points)\n",
    "\n",
    "Before we can perform BPE merges, we need to break our text into individual character tokens. This function should split a text string into a list of individual characters, with the start-of-word marker `_` **attached to the first character** of each word. Words are separated by spaces. This follows the standard convention used by tokenizers like GPT-2 (which uses `Ġ`) and SentencePiece (which uses `▁`).\n",
    "\n",
    "Arguments:\n",
    "- `text (str)`: The input text to tokenize.\n",
    "\n",
    "Returns:\n",
    "- `tokens (List[str])`: A list of character tokens where the first character of each word has `_` prefixed to it.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> tokenize_characters(\"hello\")\n",
    "['_h', 'e', 'l', 'l', 'o']\n",
    ">>> tokenize_characters(\"hi there\")\n",
    "['_h', 'i', ' ', '_t', 'h', 'e', 'r', 'e']\n",
    ">>> tokenize_characters(\"a b c\")\n",
    "['_a', ' ', '_b', ' ', '_c']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f888baf2",
   "metadata": {
    "id": "f888baf2"
   },
   "outputs": [],
   "source": [
    "def tokenize_characters(text: str) -> List[str]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    tokens = []\n",
    "    at_word_start = True\n",
    "    \n",
    "    for char in text:\n",
    "        if char == ' ':\n",
    "            tokens.append(' ')\n",
    "            at_word_start = True\n",
    "        else:\n",
    "            if at_word_start:\n",
    "                tokens.append('_' + char)\n",
    "                at_word_start = False\n",
    "            else:\n",
    "                tokens.append(char)\n",
    "    ### END CODE HERE\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115f76e7",
   "metadata": {
    "id": "115f76e7"
   },
   "source": [
    "### 2.2: Count Consecutive Pairs (3 points)\n",
    "\n",
    "A key part of BPE is identifying which pairs of tokens appear most frequently. This function takes a list of tokens and counts how many times each consecutive pair appears.\n",
    "\n",
    "Arguments:\n",
    "- `tokens (List[str])`: A list of tokens (can be characters or merged tokens).\n",
    "\n",
    "Returns:\n",
    "- `pair_counts (Dict[Tuple[str, str], int])`: A dictionary mapping each pair (as a tuple) to its frequency count.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> count_pairs(['_h', 'e', 'l', 'l', 'o'])\n",
    "{('_h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 1, ('l', 'o'): 1}\n",
    ">>> count_pairs(['_a', 'b', ' ', '_a', 'b'])\n",
    "{('_a', 'b'): 2, ('b', ' '): 1, (' ', '_a'): 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223fce2f",
   "metadata": {
    "id": "223fce2f"
   },
   "outputs": [],
   "source": [
    "def count_pairs(tokens: List[str]) -> Dict[Tuple[str, str], int]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    pair_counts = {}\n",
    "    for i in range(len(tokens) - 1):\n",
    "        pair = (tokens[i], tokens[i + 1])\n",
    "        pair_counts[pair] = pair_counts.get(pair, 0) + 1\n",
    "    ### END CODE HERE\n",
    "    return pair_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1de3a9",
   "metadata": {
    "id": "fe1de3a9"
   },
   "source": [
    "### 2.3: Find Most Frequent Pair (3 points)\n",
    "\n",
    "After counting pairs, we need to identify which pair occurs most frequently. This is the pair we'll merge in BPE. If there are no pairs (empty or single-token list), return None.\n",
    "\n",
    "Arguments:\n",
    "- `pair_counts (Dict[Tuple[str, str], int])`: A dictionary of pair counts from the previous function.\n",
    "\n",
    "Returns:\n",
    "- `most_frequent (Optional[Tuple[str, str]])`: The pair with the highest count, or None if no pairs exist.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> pairs = {('_h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 2, ('l', 'o'): 1}\n",
    ">>> find_most_frequent_pair(pairs)\n",
    "('l', 'l')\n",
    ">>> find_most_frequent_pair({})\n",
    "None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12027fbf",
   "metadata": {
    "id": "12027fbf"
   },
   "outputs": [],
   "source": [
    "def find_most_frequent_pair(pair_counts: Dict[Tuple[str, str], int]) -> Optional[Tuple[str, str]]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    if not pair_counts:\n",
    "        return None\n",
    "    most_frequent = max(pair_counts.items(), key=lambda x: x[1])[0]\n",
    "    ### END CODE HERE\n",
    "    return most_frequent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4715bf4f",
   "metadata": {
    "id": "4715bf4f"
   },
   "source": [
    "### 2.4: Merge Token Pair (3 points)\n",
    "\n",
    "Once we've identified the most frequent pair, we need to merge all occurrences of it in our token list. This function takes a list of tokens and a target pair, and replaces every consecutive occurrence of that pair with a single merged token (by concatenating the pair elements).\n",
    "\n",
    "Arguments:\n",
    "- `tokens (List[str])`: The current list of tokens.\n",
    "- `pair (Tuple[str, str])`: The pair to merge (e.g., ('e', 'l')).\n",
    "\n",
    "Returns:\n",
    "- `merged_tokens (List[str])`: A new list with all occurrences of the pair merged into single tokens.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> merge_pair(['_h', 'e', 'l', 'l', 'o'], ('l', 'l'))\n",
    "['_h', 'e', 'll', 'o']\n",
    ">>> merge_pair(['_h', 'e', 'l', 'l', 'o'], ('_h', 'e'))\n",
    "['_he', 'l', 'l', 'o']\n",
    ">>> merge_pair(['_h', 'e', 'l', 'l', 'o'], ('x', 'y'))\n",
    "['_h', 'e', 'l', 'l', 'o']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b0ec44",
   "metadata": {
    "id": "f9b0ec44"
   },
   "outputs": [],
   "source": [
    "def merge_pair(tokens: List[str], pair: Tuple[str, str]) -> List[str]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    merged_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i + 1] == pair[1]:\n",
    "            merged_tokens.append(tokens[i] + tokens[i + 1])  # merge pair\n",
    "            i += 2\n",
    "        else:\n",
    "            merged_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    ### END CODE HERE\n",
    "    return merged_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbe7e79",
   "metadata": {
    "id": "6cbe7e79"
   },
   "source": [
    "### 2.5: Build BPE Vocabulary (5 points)\n",
    "\n",
    "Now we'll put it all together! This function implements the complete BPE algorithm by iteratively finding and merging the most frequent pairs. It should:\n",
    "1. Start with character-level tokens (with word markers using `tokenize_characters`)\n",
    "2. Repeat for `num_merges` iterations:\n",
    "   - Count all pairs\n",
    "   - Find the most frequent pair\n",
    "   - Merge that pair in the token list\n",
    "3. Return the final vocabulary (unique tokens after all merges)\n",
    "\n",
    "Use the helper functions you implemented above.\n",
    "\n",
    "Arguments:\n",
    "- `text (str)`: The input text corpus to train on.\n",
    "- `num_merges (int)`: The number of merge operations to perform.\n",
    "\n",
    "Returns:\n",
    "- `vocab (List[str])`: A list of unique tokens in the final vocabulary, ordered by their frequency (most frequent first).\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> build_bpe_vocab(\"hello\", num_merges=1)\n",
    "['l', '_he', 'o', '_h', 'e']  # ('_h', 'e') merged into '_he'\n",
    ">>> build_bpe_vocab(\"hi hi\", num_merges=1)\n",
    "['_hi', ' ', '_h', 'i']  # ('_h', 'i') merged into '_hi'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250a725a",
   "metadata": {
    "id": "250a725a"
   },
   "outputs": [],
   "source": [
    "def build_bpe_vocab(text: str, num_merges: int) -> List[str]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    tokens = tokenize_characters(text)\n",
    "    \n",
    "    for _ in range(num_merges):\n",
    "        pair_counts = count_pairs(tokens)\n",
    "        most_frequent_pair = find_most_frequent_pair(pair_counts)\n",
    "        if most_frequent_pair is None:\n",
    "            break\n",
    "        tokens = merge_pair(tokens, most_frequent_pair)\n",
    "    \n",
    "    token_freq = {}\n",
    "    for token in tokens:\n",
    "        token_freq[token] = token_freq.get(token, 0) + 1\n",
    "    \n",
    "    unique_tokens = list(set(tokens))\n",
    "    vocab = sorted(unique_tokens, key=lambda x: (-token_freq[x], x))  # freq then alpha\n",
    "    ### END CODE HERE\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7876c7c",
   "metadata": {
    "id": "d7876c7c"
   },
   "source": [
    "### 2.6: Test Your Tokenizer on the Corpus [ungraded]\n",
    "\n",
    "Now let's test your BPE implementation on the Shakespeare corpus! Run the cell below to build a vocabulary with 10 merges and see what tokens are learned. The vocabulary will be ordered by token frequency (most common tokens first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6e2ea8",
   "metadata": {
    "id": "7e6e2ea8"
   },
   "outputs": [],
   "source": [
    "### DO NOT MODIFY THIS CELL ###\n",
    "# Build BPE vocabulary with 10 merges\n",
    "bpe_vocab = build_bpe_vocab(CORPUS, num_merges=10)\n",
    "\n",
    "print(f\"Vocabulary size: {len(bpe_vocab)}\")\n",
    "print(\"\\nTop 20 most frequent tokens:\")\n",
    "print(bpe_vocab[:20])\n",
    "print(\"\\nLeast 20 frequent tokens:\")\n",
    "print(bpe_vocab[-20:])\n",
    "\n",
    "# Show some interesting multi-character tokens\n",
    "multi_char = [token for token in bpe_vocab if len(token) > 1]\n",
    "print(f\"\\nNumber of multi-character tokens: {len(multi_char)}\")\n",
    "print(f\"Most common multi-character tokens: {[t for t in bpe_vocab if len(t) > 1][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6142238",
   "metadata": {
    "id": "e6142238"
   },
   "source": [
    "## 3. Calculating Relative Frequencies (15 points)\n",
    "\n",
    "This section tests your understanding of text frequency analysis, including computing word frequencies, relative frequencies, and comparing word usage across different texts. These concepts are fundamental to computational linguistics and NLP, helping us understand what makes texts distinctive and how to compare language use across different corpora.\n",
    "\n",
    "**Note on Tokenization:** For this section, we'll use a very simple tokenization approach (lowercase, split on whitespace, keep only alphabetic words). The tokenization is already handled in the provided code. **Do not modify the tokenization** - focus on implementing the frequency analysis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd5f98",
   "metadata": {
    "id": "0dcd5f98"
   },
   "source": [
    "### 3.1: Baby Corpus Setup\n",
    "\n",
    "We'll work with a small \"baby corpus\" to understand frequency calculations. The corpus contains three short texts about different topics: cooking, technology, and nature. Run the cell below to load the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f2fc6a",
   "metadata": {
    "id": "29f2fc6a"
   },
   "outputs": [],
   "source": [
    "### DO NOT MODIFY THIS CELL ###\n",
    "# Baby Corpus: Three short texts about different topics\n",
    "\n",
    "COOKING_TEXT = \"\"\"\n",
    "The chef prepared a delicious meal in the kitchen. She chopped vegetables and\n",
    "added fresh herbs to the soup. The aroma filled the kitchen as the soup simmered\n",
    "on the stove. Fresh bread was baking in the oven while the chef stirred the pot.\n",
    "The meal was ready and the chef served the delicious soup with fresh bread.\n",
    "\"\"\"\n",
    "\n",
    "TECH_TEXT = \"\"\"\n",
    "The programmer wrote code on the computer all day. She debugged the software and\n",
    "fixed several bugs in the code. The computer processed the data quickly and the\n",
    "software ran smoothly. The programmer tested the code again and the software\n",
    "worked perfectly on the computer. She saved the code and shut down the computer.\n",
    "\"\"\"\n",
    "\n",
    "NATURE_TEXT = \"\"\"\n",
    "The birds sang in the forest as the sun rose over the mountains. Flowers bloomed\n",
    "in the meadow and butterflies danced in the breeze. The river flowed through the\n",
    "forest and deer drank from the clear water. The sun warmed the forest and the\n",
    "birds continued their beautiful song in the trees.\n",
    "\"\"\"\n",
    "\n",
    "# Simple tokenizer: lowercase and split on whitespace, keeping only alphabetic tokens\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Tokenize text into lowercase alphabetic words.\"\"\"\n",
    "    return [word.lower() for word in text.split() if word.isalpha()]\n",
    "\n",
    "# Pre-tokenized texts\n",
    "cooking_tokens = simple_tokenize(COOKING_TEXT)\n",
    "tech_tokens = simple_tokenize(TECH_TEXT)\n",
    "nature_tokens = simple_tokenize(NATURE_TEXT)\n",
    "\n",
    "print(f\"Cooking text: {len(cooking_tokens)} tokens\")\n",
    "print(f\"Tech text: {len(tech_tokens)} tokens\")\n",
    "print(f\"Nature text: {len(nature_tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb0b26f",
   "metadata": {
    "id": "5fb0b26f"
   },
   "source": [
    "#### 3.1.1: Count Word Frequencies (2 points)\n",
    "\n",
    "This function counts how many times each word appears in a list of tokens and returns a dictionary mapping words to their counts (raw frequencies).\n",
    "\n",
    "Arguments:\n",
    "- `tokens (List[str])`: A list of word tokens.\n",
    "\n",
    "Returns:\n",
    "- `freq_dict (Dict[str, int])`: A dictionary where keys are words and values are their counts.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> count_frequencies([\"the\", \"cat\", \"sat\", \"the\"])\n",
    "{'the': 2, 'cat': 1, 'sat': 1}\n",
    ">>> count_frequencies([\"hello\", \"hello\", \"world\"])\n",
    "{'hello': 2, 'world': 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb72fc84",
   "metadata": {
    "id": "eb72fc84"
   },
   "outputs": [],
   "source": [
    "def count_frequencies(tokens: List[str]) -> Dict[str, int]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    freq_dict = {}\n",
    "    for token in tokens:\n",
    "        freq_dict[token] = freq_dict.get(token, 0) + 1\n",
    "    ### END CODE HERE\n",
    "    return freq_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192efec5",
   "metadata": {
    "id": "192efec5"
   },
   "source": [
    "#### 3.1.2: Compute Relative Frequencies (2 points)\n",
    "\n",
    "Raw counts don't allow fair comparison between texts of different lengths. **Relative frequency** (or normalized frequency) tells us what proportion of the text is made up of each word.\n",
    "\n",
    "**Relative Frequency** = (Word Count) / (Total Words)\n",
    "\n",
    "This function takes a list of tokens, computes their frequencies (by calling `count_frequencies`), and then converts those counts to relative frequencies.\n",
    "\n",
    "Arguments:\n",
    "- `tokens (List[str])`: A list of word tokens.\n",
    "\n",
    "Returns:\n",
    "- `rel_freq_dict (Dict[str, float])`: A dictionary where keys are words and values are their relative frequencies (between 0 and 1).\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> compute_relative_frequencies(['the', 'cat', 'sat', 'on', 'the', 'mat'])\n",
    "{'the': 0.3333333333333333, 'cat': 0.16666666666666666, 'sat': 0.16666666666666666, 'on': 0.16666666666666666, 'mat': 0.16666666666666666}\n",
    ">>> compute_relative_frequencies(['hello', 'world'])\n",
    "{'hello': 0.5, 'world': 0.5}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124df6ff",
   "metadata": {
    "id": "124df6ff"
   },
   "outputs": [],
   "source": [
    "def compute_relative_frequencies(tokens: List[str]) -> Dict[str, float]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    freq_dict = count_frequencies(tokens)\n",
    "    total = len(tokens)\n",
    "    rel_freq_dict = {}\n",
    "    for word, count in freq_dict.items():\n",
    "        rel_freq_dict[word] = count / total\n",
    "    ### END CODE HERE\n",
    "    return rel_freq_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282f851",
   "metadata": {
    "id": "e282f851"
   },
   "source": [
    "#### 3.1.3: Get Top N Words (3 points)\n",
    "\n",
    "This function returns the top N most frequent words from a frequency dictionary, sorted by frequency in descending order. If there are ties, sort alphabetically as a tiebreaker.\n",
    "\n",
    "Arguments:\n",
    "- `freq_dict (Dict[str, int])`: A dictionary mapping words to their counts.\n",
    "- `n (int)`: The number of top words to return.\n",
    "\n",
    "Returns:\n",
    "- `top_words (List[Tuple[str, int]])`: A list of (word, count) tuples for the top N words, sorted by count (descending), then alphabetically for ties.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> get_top_n_words({'the': 5, 'cat': 3, 'sat': 3, 'on': 1}, 2)\n",
    "[('the', 5), ('cat', 3)]\n",
    ">>> get_top_n_words({'a': 2, 'b': 2, 'c': 2}, 2)\n",
    "[('a', 2), ('b', 2)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9babbae",
   "metadata": {
    "id": "e9babbae"
   },
   "outputs": [],
   "source": [
    "def get_top_n_words(freq_dict: Dict[str, int], n: int) -> List[Tuple[str, int]]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    # Sort by count (descending), then alphabetically for ties\n",
    "    sorted_words = sorted(freq_dict.items(), key=lambda x: (-x[1], x[0]))\n",
    "    top_words = sorted_words[:n]\n",
    "    ### END CODE HERE\n",
    "    return top_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f98d381",
   "metadata": {
    "id": "8f98d381"
   },
   "source": [
    "#### 3.1.4: Compare Word Frequency (2 points)\n",
    "\n",
    "This function compares how a specific word is used across two texts by computing its relative frequency in each. This helps us understand which text uses the word more prominently.\n",
    "\n",
    "Arguments:\n",
    "- `word (str)`: The word to compare.\n",
    "- `freq_dict1 (Dict[str, int])`: Frequency dictionary for the first text.\n",
    "- `total1 (int)`: Total tokens in the first text.\n",
    "- `freq_dict2 (Dict[str, int])`: Frequency dictionary for the second text.\n",
    "- `total2 (int)`: Total tokens in the second text.\n",
    "\n",
    "Returns:\n",
    "- `comparison (Tuple[float, float])`: A tuple of (relative_freq_in_text1, relative_freq_in_text2). If a word doesn't appear in a text, its relative frequency should be 0.0.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> compare_word_frequency(\"the\", freq_dict1={'the': 10}, total1=100, freq_dict2={'the': 5}, total2=50)\n",
    "(0.1, 0.1)\n",
    ">>> compare_word_frequency(\"cat\", freq_dict1={'cat': 5}, total1=100, freq_dict2={'dog': 5}, total2=100)\n",
    "(0.05, 0.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34042ddf",
   "metadata": {
    "id": "34042ddf"
   },
   "outputs": [],
   "source": [
    "def compare_word_frequency(word: str, freq_dict1: Dict[str, int], total1: int,\n",
    "                           freq_dict2: Dict[str, int], total2: int) -> Tuple[float, float]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    # relative frequency in text1\n",
    "    count1 = freq_dict1.get(word, 0)\n",
    "    rel_freq1 = count1 / total1 if total1 > 0 else 0.0\n",
    "    \n",
    "    # relative frequency in text2\n",
    "    count2 = freq_dict2.get(word, 0)\n",
    "    rel_freq2 = count2 / total2 if total2 > 0 else 0.0\n",
    "    \n",
    "    comparison = (rel_freq1, rel_freq2)\n",
    "    ### END CODE HERE\n",
    "    return comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd494a5",
   "metadata": {
    "id": "fbd494a5"
   },
   "source": [
    "#### 3.1.5: Calculate Frequency Ratio (3 points)\n",
    "\n",
    "A **frequency ratio** tells us how many times more frequent a word is in one text compared to another. This is useful for identifying **keywords** - words that are characteristic of a particular text.\n",
    "\n",
    "**Frequency Ratio** = (Relative Frequency in Target) / (Relative Frequency in Reference)\n",
    "\n",
    "To avoid division by zero when a word doesn't appear in the reference text, use smoothing: if the reference frequency is 0, use a small value (0.5 / total_reference_tokens) instead.\n",
    "\n",
    "Arguments:\n",
    "- `word (str)`: The word to analyze.\n",
    "- `target_freq (Dict[str, int])`: Frequency dictionary for the target text.\n",
    "- `target_total (int)`: Total tokens in the target text.\n",
    "- `reference_freq (Dict[str, int])`: Frequency dictionary for the reference text.\n",
    "- `reference_total (int)`: Total tokens in the reference text.\n",
    "\n",
    "Returns:\n",
    "- `ratio (float)`: The frequency ratio. Returns 0.0 if the word doesn't appear in the target text.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> calculate_frequency_ratio(\"whale\", target_freq={'whale': 10}, target_total=100, reference_freq={'whale': 1}, reference_total=100)\n",
    "10.0\n",
    ">>> calculate_frequency_ratio(\"computer\", target_freq={'computer': 5}, target_total=100, reference_freq={}, reference_total=100)\n",
    "10.0  # Uses smoothed reference frequency\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da642e2a",
   "metadata": {
    "id": "da642e2a"
   },
   "outputs": [],
   "source": [
    "def calculate_frequency_ratio(word: str, target_freq: Dict[str, int], target_total: int,\n",
    "                              reference_freq: Dict[str, int], reference_total: int) -> float:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    # word not in target -> return 0.0\n",
    "    if word not in target_freq:\n",
    "        return 0.0\n",
    "    \n",
    "    target_rel_freq = target_freq[word] / target_total if target_total > 0 else 0.0\n",
    "    reference_count = reference_freq.get(word, 0)\n",
    "    reference_rel_freq = reference_count / reference_total if reference_total > 0 else 0.0\n",
    "    \n",
    "    # reference frequency is 0 -> use smoothing\n",
    "    if reference_rel_freq == 0.0:\n",
    "        reference_rel_freq = 0.5 / reference_total if reference_total > 0 else 0.5\n",
    "    \n",
    "    ratio = target_rel_freq / reference_rel_freq if reference_rel_freq > 0 else 0.0\n",
    "    ### END CODE HERE\n",
    "    return ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf74050",
   "metadata": {
    "id": "4bf74050"
   },
   "source": [
    "#### 3.1.6: Find Keywords (3 points)\n",
    "\n",
    "Keywords are words that appear significantly more often in a target text compared to a reference text. This function finds all words in the target text that have a frequency ratio above a given threshold.\n",
    "\n",
    "Arguments:\n",
    "- `target_freq (Dict[str, int])`: Frequency dictionary for the target text.\n",
    "- `target_total (int)`: Total tokens in the target text.\n",
    "- `reference_freq (Dict[str, int])`: Frequency dictionary for the reference text.\n",
    "- `reference_total (int)`: Total tokens in the reference text.\n",
    "- `min_ratio (float)`: Minimum frequency ratio to consider a word as a keyword.\n",
    "\n",
    "Returns:\n",
    "- `keywords (List[Tuple[str, float]])`: A list of (word, ratio) tuples for words with ratio >= min_ratio, sorted by ratio in descending order.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> find_keywords(target_freq={'whale': 10, 'the': 5}, target_total=100, reference_freq={'whale': 1, 'the': 10}, reference_total=100, min_ratio=2.0)\n",
    "[('whale', 10.0)]\n",
    ">>> find_keywords(target_freq={'code': 8, 'and': 4}, target_total=100, reference_freq={'code': 2, 'and': 4}, reference_total=100, min_ratio=3.0)\n",
    "[('code', 4.0)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59696803",
   "metadata": {
    "id": "59696803"
   },
   "outputs": [],
   "source": [
    "def find_keywords(target_freq: Dict[str, int], target_total: int,\n",
    "                  reference_freq: Dict[str, int], reference_total: int,\n",
    "                  min_ratio: float) -> List[Tuple[str, float]]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    keywords = []\n",
    "    for word in target_freq:\n",
    "        ratio = calculate_frequency_ratio(word, target_freq, target_total,\n",
    "                                         reference_freq, reference_total)\n",
    "        if ratio >= min_ratio:\n",
    "            keywords.append((word, ratio))\n",
    "    # descending sort by ratio\n",
    "    keywords.sort(key=lambda x: -x[1])\n",
    "    ### END CODE HERE\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3ab6c",
   "metadata": {
    "id": "20e3ab6c"
   },
   "source": [
    "#### 3.1.7: Test Your Frequency Functions [ungraded]\n",
    "\n",
    "Run the cell below to test your implementations on the baby corpus. This will show you the frequency analysis results for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b94250",
   "metadata": {
    "id": "37b94250"
   },
   "outputs": [],
   "source": [
    "### DO NOT MODIFY THIS CELL ###\n",
    "# Test your frequency functions on the baby corpus\n",
    "\n",
    "# Count frequencies for each text\n",
    "cooking_freq = count_frequencies(cooking_tokens)\n",
    "tech_freq = count_frequencies(tech_tokens)\n",
    "nature_freq = count_frequencies(nature_tokens)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FREQUENCY ANALYSIS OF BABY CORPUS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show top 5 words from each text\n",
    "print(\"\\n--- Top 5 Words in Each Text ---\")\n",
    "print(f\"Cooking: {get_top_n_words(cooking_freq, 5)}\")\n",
    "print(f\"Tech: {get_top_n_words(tech_freq, 5)}\")\n",
    "print(f\"Nature: {get_top_n_words(nature_freq, 5)}\")\n",
    "\n",
    "# Compare 'the' across texts\n",
    "print(\"\\n--- Comparing 'the' across texts ---\")\n",
    "cooking_the, tech_the = compare_word_frequency(\"the\", cooking_freq, len(cooking_tokens),\n",
    "                                                tech_freq, len(tech_tokens))\n",
    "print(f\"'the' in Cooking: {cooking_the:.4f} ({cooking_the*100:.2f}%)\")\n",
    "print(f\"'the' in Tech: {tech_the:.4f} ({tech_the*100:.2f}%)\")\n",
    "\n",
    "# Find keywords in cooking text vs tech text\n",
    "print(\"\\n--- Keywords in Cooking (vs Tech) with ratio >= 2.0 ---\")\n",
    "cooking_keywords = find_keywords(cooking_freq, len(cooking_tokens),\n",
    "                                  tech_freq, len(tech_tokens), 2.0)\n",
    "for word, ratio in cooking_keywords[:5]:\n",
    "    print(f\"  {word}: {ratio:.2f}x more frequent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64a25a1",
   "metadata": {
    "id": "e64a25a1"
   },
   "source": [
    "## 4: Collocations and PMI Analysis (17 points)\n",
    "\n",
    "**Collocations** are word pairs that appear together more frequently than we'd expect by chance. While frequency analysis (Section 3) looks at individual words, collocation analysis examines word associations.\n",
    "\n",
    "**Pointwise Mutual Information (PMI)** measures the strength of association between two words by comparing their co-occurrence probability with their independent probabilities.\n",
    "\n",
    "**PMI Formula**: PMI(x, y) = log₂(P(x,y) / (P(x) × P(y)))\n",
    "\n",
    "- **High PMI**: Words appear together much more than expected → strong collocation\n",
    "- **PMI ≈ 0**: Words appear together about as expected by chance\n",
    "- **Negative PMI**: Words appear together less than expected\n",
    "\n",
    "In this section, you'll implement functions to find and analyze collocations using PMI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51de6ee",
   "metadata": {
    "id": "a51de6ee"
   },
   "source": [
    "### 4.1: PMI Collocation Functions\n",
    "\n",
    "Implement the following functions to analyze collocations using PMI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87551694",
   "metadata": {
    "id": "87551694"
   },
   "source": [
    "#### 4.1.1: Generate Bigrams (2 points)\n",
    "\n",
    "A bigram is a sequence of two consecutive words. This function generates all bigrams from a list of tokens.\n",
    "\n",
    "Arguments:\n",
    "- `tokens (List[str])`: A list of word tokens.\n",
    "\n",
    "Returns:\n",
    "- `bigrams (List[Tuple[str, str]])`: A list of bigrams, where each bigram is a tuple of two consecutive words.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> generate_bigrams([\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"])\n",
    "[('the', 'cat'), ('cat', 'sat'), ('sat', 'on'), ('on', 'the'), ('the', 'mat')]\n",
    ">>> generate_bigrams([\"hello\", \"world\"])\n",
    "[('hello', 'world')]\n",
    ">>> generate_bigrams([\"single\"])\n",
    "[]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecfd924",
   "metadata": {
    "id": "fecfd924"
   },
   "outputs": [],
   "source": [
    "def generate_bigrams(tokens: List[str]) -> List[Tuple[str, str]]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    bigrams = []\n",
    "    # tuples of consecutive word pairs\n",
    "    for i in range(len(tokens) - 1):\n",
    "        bigrams.append((tokens[i], tokens[i + 1]))\n",
    "    ### END CODE HERE\n",
    "    return bigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef84b9c8",
   "metadata": {
    "id": "ef84b9c8"
   },
   "source": [
    "#### 4.1.2: Count Bigram Frequencies (2 points)\n",
    "\n",
    "This function counts how many times each bigram appears in a list of bigrams and returns a dictionary mapping bigrams to their counts.\n",
    "\n",
    "Arguments:\n",
    "- `bigrams (List[Tuple[str, str]])`: A list of bigrams.\n",
    "\n",
    "Returns:\n",
    "- `bigram_freq (Dict[Tuple[str, str], int])`: A dictionary where keys are bigrams (tuples) and values are their counts.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> count_bigram_frequencies([('the', 'cat'), ('the', 'dog'), ('the', 'cat')])\n",
    "{('the', 'cat'): 2, ('the', 'dog'): 1}\n",
    ">>> count_bigram_frequencies([('hello', 'world'), ('hello', 'world')])\n",
    "{('hello', 'world'): 2}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32c27d8",
   "metadata": {
    "id": "b32c27d8"
   },
   "outputs": [],
   "source": [
    "def count_bigram_frequencies(bigrams: List[Tuple[str, str]]) -> Dict[Tuple[str, str], int]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    bigram_freq = {}\n",
    "    for bigram in bigrams:\n",
    "        bigram_freq[bigram] = bigram_freq.get(bigram, 0) + 1\n",
    "    ### END CODE HERE\n",
    "    return bigram_freq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbbe5a5",
   "metadata": {
    "id": "8fbbe5a5"
   },
   "source": [
    "#### 4.1.3: Calculate PMI Score (4 points)\n",
    "\n",
    "This function calculates the PMI (Pointwise Mutual Information) score for a single bigram.\n",
    "\n",
    "**PMI Formula**: PMI(x, y) = log₂(P(x,y) / (P(x) × P(y)))\n",
    "\n",
    "Where:\n",
    "- P(x,y) = bigram_count / total_bigrams (probability of bigram occurring)\n",
    "- P(x) = word1_count / total_words (probability of first word)\n",
    "- P(y) = word2_count / total_words (probability of second word)\n",
    "\n",
    "Arguments:\n",
    "- `bigram (Tuple[str, str])`: The bigram to analyze.\n",
    "- `bigram_count (int)`: How many times this bigram appears.\n",
    "- `word1_count (int)`: How many times the first word appears.\n",
    "- `word2_count (int)`: How many times the second word appears.\n",
    "- `total_bigrams (int)`: Total number of bigrams in the corpus.\n",
    "- `total_words (int)`: Total number of words in the corpus.\n",
    "\n",
    "Returns:\n",
    "- `pmi (float)`: The PMI score for this bigram.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> calculate_pmi(('strong', 'coffee'), 10, 50, 100, 1000, 1001)\n",
    "# P(strong,coffee) = 10/1000 = 0.01\n",
    "# P(strong) = 50/1001 ≈ 0.05, P(coffee) = 100/1001 ≈ 0.1\n",
    "# PMI = log2(0.01 / (0.05 * 0.1)) ≈ 0.997\n",
    "0.9965784284662086\n",
    "```\n",
    "\n",
    "**Note**: Use `math.log2()` for the logarithm. `math` is already at the imported at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1479afe3",
   "metadata": {
    "id": "1479afe3"
   },
   "outputs": [],
   "source": [
    "def calculate_pmi(bigram: Tuple[str, str], bigram_count: int, word1_count: int,\n",
    "                  word2_count: int, total_bigrams: int, total_words: int) -> float:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    # P(x,y) = bigram_count / total_bigrams\n",
    "    p_xy = bigram_count / total_bigrams if total_bigrams > 0 else 0.0\n",
    "    \n",
    "    # P(x) = word1_count / total_words\n",
    "    p_x = word1_count / total_words if total_words > 0 else 0.0\n",
    "    \n",
    "    # P(y) = word2_count / total_words\n",
    "    p_y = word2_count / total_words if total_words > 0 else 0.0\n",
    "    \n",
    "    # PMI = log2(P(x,y) / (P(x) * P(y)))\n",
    "    if p_x * p_y == 0:\n",
    "        pmi = 0.0  # avoid division by zero\n",
    "    else:\n",
    "        pmi = math.log2(p_xy / (p_x * p_y))\n",
    "    ### END CODE HERE\n",
    "    return pmi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd43c045",
   "metadata": {
    "id": "fd43c045"
   },
   "source": [
    "#### 4.1.4: Calculate PMI for All Bigrams (3 points)\n",
    "\n",
    "This function calculates PMI scores for all bigrams in a corpus and returns them sorted by PMI score in descending order.\n",
    "\n",
    "Arguments:\n",
    "- `bigram_freq (Dict[Tuple[str, str], int])`: Dictionary of bigram frequencies.\n",
    "- `word_freq (Dict[str, int])`: Dictionary of word (unigram) frequencies.\n",
    "- `total_bigrams (int)`: Total number of bigrams.\n",
    "- `total_words (int)`: Total number of words.\n",
    "\n",
    "Returns:\n",
    "- `pmi_scores (List[Tuple[Tuple[str, str], float, int]])`: A list of tuples, where each tuple contains (bigram, pmi_score, frequency), sorted by PMI score in descending order.\n",
    "\n",
    "Example:\n",
    "```python\n",
    ">>> bigram_freq = {('strong', 'coffee'): 10, ('the', 'cat'): 5}\n",
    ">>> word_freq = {'strong': 50, 'coffee': 100, 'the': 200, 'cat': 30}\n",
    ">>> calculate_all_pmi(bigram_freq, word_freq, 1000, 1001)\n",
    "# Returns list with (bigram, pmi, count) tuples sorted by PMI\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b78e01",
   "metadata": {
    "id": "97b78e01"
   },
   "outputs": [],
   "source": [
    "def calculate_all_pmi(bigram_freq: Dict[Tuple[str, str], int], word_freq: Dict[str, int],\n",
    "                     total_bigrams: int, total_words: int) -> List[Tuple[Tuple[str, str], float, int]]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    pmi_scores = []\n",
    "    for bigram, bigram_count in bigram_freq.items():\n",
    "        word1, word2 = bigram\n",
    "        word1_count = word_freq.get(word1, 0)\n",
    "        word2_count = word_freq.get(word2, 0)\n",
    "        pmi = calculate_pmi(bigram, bigram_count, word1_count, word2_count, total_bigrams, total_words)\n",
    "        pmi_scores.append((bigram, pmi, bigram_count))\n",
    "    # sort by PMI score - descending\n",
    "    pmi_scores.sort(key=lambda x: -x[1])\n",
    "    ### END CODE HERE\n",
    "    return pmi_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e856f58",
   "metadata": {
    "id": "8e856f58"
   },
   "source": [
    "#### 4.1.5: Filter Collocations by Minimum Frequency (2 points)\n",
    "\n",
    "PMI can give very high scores to rare bigrams that appear only once or twice. To find meaningful collocations, we need to filter by minimum frequency.\n",
    "\n",
    "This function filters a list of PMI scores to only include bigrams that appear at least `min_freq` times.\n",
    "\n",
    "Arguments:\n",
    "- `pmi_scores (List[Tuple[Tuple[str, str], float, int]])`: List of (bigram, pmi, frequency) tuples.\n",
    "- `min_freq (int)`: Minimum frequency threshold.\n",
    "\n",
    "Returns:\n",
    "- `filtered (List[Tuple[Tuple[str, str], float, int]])`: Filtered list containing only bigrams with frequency >= min_freq.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> pmi_scores = [(('white', 'whale'), 8.5, 50), (('rare', 'word'), 12.0, 1), (('the', 'cat'), 3.2, 100)]\n",
    ">>> filter_by_frequency(pmi_scores, 10)\n",
    "[(('white', 'whale'), 8.5, 50), (('the', 'cat'), 3.2, 100)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c5d378",
   "metadata": {
    "id": "b3c5d378"
   },
   "outputs": [],
   "source": [
    "def filter_by_frequency(pmi_scores: List[Tuple[Tuple[str, str], float, int]],\n",
    "                       min_freq: int) -> List[Tuple[Tuple[str, str], float, int]]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    # keep only bigrams with frequency >= min_freq\n",
    "    filtered = [item for item in pmi_scores if item[2] >= min_freq]\n",
    "    ### END CODE HERE\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af86f9",
   "metadata": {
    "id": "84af86f9"
   },
   "source": [
    "#### 4.1.6: Find Collocations for a Specific Word (4 points)\n",
    "\n",
    "This function finds all collocations containing a specific target word, showing which words commonly appear with it and whether they appear to the left or right.\n",
    "\n",
    "Arguments:\n",
    "- `target_word (str)`: The word to find collocations for.\n",
    "- `pmi_scores (List[Tuple[Tuple[str, str], float, int]])`: List of (bigram, pmi, frequency) tuples.\n",
    "- `min_freq (int)`: Minimum frequency threshold (default 1).\n",
    "\n",
    "Returns:\n",
    "- `collocations (List[Tuple[str, float, int, str]])`: A list of tuples (collocate_word, pmi, frequency, position), where position is either 'left' or 'right', sorted by PMI in descending order.\n",
    "\n",
    "Example:\n",
    "```python\n",
    ">>> pmi_scores = [(('white', 'whale'), 8.5, 50), (('whale', 'ship'), 7.2, 30), (('the', 'whale'), 2.1, 100)]\n",
    ">>> find_word_collocations('whale', pmi_scores, min_freq=10)\n",
    "[('white', 8.5, 50, 'left'), ('ship', 7.2, 30, 'right'), ('the', 2.1, 100, 'left')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25133b25",
   "metadata": {
    "id": "25133b25"
   },
   "outputs": [],
   "source": [
    "def find_word_collocations(target_word: str, pmi_scores: List[Tuple[Tuple[str, str], float, int]],\n",
    "                          min_freq: int = 1) -> List[Tuple[str, float, int, str]]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    collocations = []\n",
    "    for bigram, pmi, frequency in pmi_scores:\n",
    "        # check if frequency meets minimum threshold\n",
    "        if frequency < min_freq:\n",
    "            continue\n",
    "        \n",
    "        word1, word2 = bigram\n",
    "        # check if target_word is in the bigram\n",
    "        if word1 == target_word:\n",
    "            # target_word: left, collocate: right\n",
    "            collocations.append((word2, pmi, frequency, 'right'))\n",
    "        elif word2 == target_word:\n",
    "            # target_word: right, collocate: left\n",
    "            collocations.append((word1, pmi, frequency, 'left'))\n",
    "    \n",
    "    # sort by PMI - descending\n",
    "    collocations.sort(key=lambda x: -x[1])\n",
    "    ### END CODE HERE\n",
    "    return collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea649a2",
   "metadata": {
    "id": "bea649a2"
   },
   "source": [
    "## 5. Building a Naive Bayes Text Classifier from Scratch (21 points)\n",
    "\n",
    "In this section, you will implement a **Naive Bayes text classifier** from scratch for news article classification. Naive Bayes is a probabilistic classifier based on Bayes' theorem with the \"naive\" assumption that features (words) are conditionally independent given the class.\n",
    "\n",
    "**Bayes' Theorem:**\n",
    "$$P(class|document) = \\frac{P(document|class) \\times P(class)}{P(document)}$$\n",
    "\n",
    "Since $P(document)$ is the same for all classes, we can simplify to:\n",
    "$$P(class|document) \\propto P(document|class) \\times P(class)$$\n",
    "\n",
    "**Naive Assumption:** We assume words are independent given the class:\n",
    "$$P(document|class) = \\prod_{word \\in document} P(word|class)$$\n",
    "\n",
    "To avoid numerical underflow with many word probabilities, we use **log probabilities**:\n",
    "$$\\log P(class|document) = \\log P(class) + \\sum_{word \\in document} \\log P(word|class)$$\n",
    "\n",
    "**Laplace Smoothing:** To handle words not seen in training, we add a small constant (usually 1) to all word counts:\n",
    "$$P(word|class) = \\frac{count(word, class) + 1}{total\\_words\\_in\\_class + vocabulary\\_size}$$\n",
    "\n",
    "You will build a classifier to categorize news articles into categories like: **tech**, **sports**, **business**, **health**, and **politics**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c1ac7b",
   "metadata": {
    "id": "83c1ac7b"
   },
   "source": [
    "### 5.1: Load and Explore the Dataset\n",
    "\n",
    "First, let's load the news classification dataset and explore its structure. The dataset contains news article text snippets and their corresponding categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bb5532",
   "metadata": {
    "id": "01bb5532"
   },
   "outputs": [],
   "source": [
    "### DO NOT MODIFY THIS CELL ###\n",
    "\n",
    "def load_news_dataset():\n",
    "    df = pd.read_csv(\"https://huggingface.co/datasets/okite97/news-data/raw/main/train.csv\")\n",
    "    df = df.rename(columns={'Title': 'title', 'Excerpt': 'text', 'Category': 'category'})\n",
    "    # stratified sampling to work with smaller dataset\n",
    "    n_samples = 500\n",
    "    # Sample proportionally from each category\n",
    "    sampled_indices = df.groupby('category').apply(\n",
    "        lambda x: x.sample(n=int(n_samples * len(x) / len(df)), random_state=42).index,\n",
    "        include_groups=False\n",
    "    )\n",
    "    # flatten the indices and select rows\n",
    "    all_indices = [idx for indices in sampled_indices for idx in indices]\n",
    "    df = df.loc[all_indices]\n",
    "    df = df.sample(frac=1, random_state=42)\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "news_df = load_news_dataset()\n",
    "print(\"Dataset Shape:\", news_df.shape)\n",
    "print(\"\\nColumn Names:\", news_df.columns.tolist())\n",
    "print(\"\\nCategory Distribution:\")\n",
    "print(news_df['category'].value_counts())\n",
    "print(\"\\nSample articles:\")\n",
    "print(news_df[['title', 'category']].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad393af6",
   "metadata": {
    "id": "ad393af6"
   },
   "source": [
    "Below we perform a crucial step in machine learning: splitting our dataset into training and test sets using an 80/20 ratio.\n",
    "\n",
    "**Why is this necessary?** To properly evaluate our classifier, we need to test it on data it has never seen before. If we trained and tested on the same data, we couldn't tell if the model truly learned patterns or just memorized the training examples. The test set simulates real-world usage where the model encounters new, unseen text.\n",
    "\n",
    "**The approach:** We use a simple split where the first 80% of documents become the training set (used to learn word probabilities and patterns) and the remaining 20% become the test set (used to evaluate performance). The texts and labels are extracted into separate lists for convenience in later processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f563f",
   "metadata": {
    "id": "8f8f563f"
   },
   "outputs": [],
   "source": [
    "### DO NOT MODIFY THIS CELL ###\n",
    "split_index = int(0.8 * len(news_df))\n",
    "train_df = news_df[:split_index].reset_index(drop=True)\n",
    "test_df = news_df[split_index:].reset_index(drop=True)\n",
    "\n",
    "train_texts = train_df['text'].tolist()\n",
    "train_labels = train_df['category'].tolist()\n",
    "test_texts = test_df['text'].tolist()\n",
    "test_labels = test_df['category'].tolist()\n",
    "\n",
    "print(f\"\\nTraining set size: {len(train_texts)}\")\n",
    "print(f\"Testing set size: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486162d",
   "metadata": {
    "id": "5486162d"
   },
   "source": [
    "### 5.2: Preprocess Text (2 points)\n",
    "\n",
    "Before building our classifier, we need to tokenize the text (convert to lowercase and split into words).\n",
    "\n",
    "#### 5.2.1: Implement Text Preprocessing (2 points)\n",
    "\n",
    "Implement a function that takes raw text and returns a list of lowercase word tokens. Convert the text to lowercase, replace all non-alphabetic characters (except spaces) with empty strings, split on whitespace, and filter out empty strings.\n",
    "\n",
    "Arguments:\n",
    "- `text (str)`: The raw text to preprocess.\n",
    "\n",
    "Returns:\n",
    "- `tokens (List[str])`: A list of lowercase word tokens.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> preprocess_text(\"Hello World! This is NLP.\")\n",
    "['hello', 'world', 'this', 'is', 'nlp']\n",
    ">>> preprocess_text(\"AI and ML are cool!\")\n",
    "['ai', 'and', 'ml', 'are', 'cool']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c13419",
   "metadata": {
    "id": "08c13419"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> List[str]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "\n",
    "    text = text.lower()\n",
    "    # all non-alphabetic characters except spaces -> empty strings\n",
    "    text = re.sub(r'[^a-z ]', '', text)\n",
    "    \n",
    "    tokens = [word for word in text.split() if word]\n",
    "    ### END CODE HERE\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04e9fb4",
   "metadata": {
    "id": "a04e9fb4"
   },
   "source": [
    "### 5.3: Train the Naive Bayes Classifier (11 points)\n",
    "\n",
    "Now we'll implement the training phase of our Naive Bayes classifier. Training involves computing:\n",
    "\n",
    "1. **Prior probabilities**: $P(class)$ - the probability of each class based on training data\n",
    "2. **Likelihood probabilities**: $P(word|class)$ - the probability of each word given a class\n",
    "3. **Vocabulary**: the set of all unique words in the training data\n",
    "\n",
    "#### 5.3.1: Compute Prior Probabilities (3 points)\n",
    "\n",
    "Implement a function that computes the prior probability for each class, which represents the fraction of training documents belonging to each category.\n",
    "\n",
    "**The approach:** For each unique class label in the training data, count how many documents belong to that class, then divide by the total number of training documents. This gives us $P(class)$, which represents our baseline expectation of seeing each category before looking at the document's content.\n",
    "\n",
    "Arguments:\n",
    "- `labels (List[str])`: List of class labels from training data.\n",
    "\n",
    "Returns:\n",
    "- `priors (Dict[str, float])`: Dictionary mapping each class to its prior probability.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> labels = [\"tech\", \"tech\", \"sports\", \"sports\", \"tech\"]\n",
    ">>> compute_priors(labels)\n",
    "{'tech': 0.6, 'sports': 0.4}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdf6321",
   "metadata": {
    "id": "cbdf6321"
   },
   "outputs": [],
   "source": [
    "def compute_priors(labels: List[str]) -> Dict[str, float]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    priors = {}\n",
    "    total = len(labels)\n",
    "    # documents per class\n",
    "    for label in labels:\n",
    "        priors[label] = priors.get(label, 0) + 1\n",
    "    \n",
    "    # counts --> probabilities conversion\n",
    "    for label in priors:\n",
    "        priors[label] = priors[label] / total\n",
    "    ### END CODE HERE\n",
    "    return priors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e60c58",
   "metadata": {
    "id": "34e60c58"
   },
   "source": [
    "#### 5.3.2: Compute Word Counts per Class (3 points)\n",
    "\n",
    "Implement a function that builds the word frequency statistics needed for computing likelihood probabilities in Naive Bayes classification.\n",
    "\n",
    "**The approach:** This function processes all training documents to create a comprehensive statistical profile for each class. It tracks three key pieces of information:\n",
    "1. How many times each word appears in documents of each class (word counts per class)\n",
    "2. The total number of words in all documents of each class (for normalization)\n",
    "3. The complete vocabulary of unique words across all training documents\n",
    "\n",
    "Arguments:\n",
    "- `texts (List[str])`: List of text documents.\n",
    "- `labels (List[str])`: Corresponding list of class labels.\n",
    "\n",
    "Returns:\n",
    "- `word_counts (Dict[str, Dict[str, int]])`: Nested dictionary where `word_counts[class][word]` gives the count of `word` in documents of `class`.\n",
    "- `class_total_words (Dict[str, int])`: Dictionary mapping each class to its total word count.\n",
    "- `vocabulary (set)`: Set of all unique words across all documents.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> texts = [\"hello world\", \"hello there\"]\n",
    ">>> labels = [\"a\", \"b\"]\n",
    ">>> word_counts, class_totals, vocab = compute_word_counts(texts, labels)\n",
    ">>> word_counts['a']['hello']\n",
    "1\n",
    ">>> class_totals['a']\n",
    "2\n",
    ">>> 'world' in vocab\n",
    "True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3d4dc",
   "metadata": {
    "id": "fda3d4dc"
   },
   "outputs": [],
   "source": [
    "def compute_word_counts(texts: List[str], labels: List[str]) -> Tuple[Dict[str, Dict[str, int]], Dict[str, int], set]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    word_counts = {}\n",
    "    class_total_words = {}\n",
    "    vocabulary = set()\n",
    "    \n",
    "    for text, label in zip(texts, labels):\n",
    "        tokens = preprocess_text(text)\n",
    "        \n",
    "        if label not in word_counts:\n",
    "            word_counts[label] = {}\n",
    "        if label not in class_total_words:\n",
    "            class_total_words[label] = 0\n",
    "        \n",
    "        for word in tokens:\n",
    "            word_counts[label][word] = word_counts[label].get(word, 0) + 1\n",
    "            class_total_words[label] += 1\n",
    "            vocabulary.add(word)\n",
    "    ### END CODE HERE\n",
    "    return word_counts, class_total_words, vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912e3486",
   "metadata": {
    "id": "912e3486"
   },
   "source": [
    "#### 5.3.3: Compute Word Likelihood with Laplace Smoothing (5 points)\n",
    "\n",
    "Implement a function that computes the log probability of a word given a class, using Laplace (add-1) smoothing to handle unseen words.\n",
    "\n",
    "**Formula with Laplace Smoothing:**\n",
    "$$P(word|class) = \\frac{count(word, class) + 1}{total\\_words\\_in\\_class + vocabulary\\_size}$$\n",
    "\n",
    "We return the **log probability** to avoid underflow issues when multiplying many small probabilities.\n",
    "\n",
    "Arguments:\n",
    "- `word (str)`: The word to compute probability for.\n",
    "- `class_label (str)`: The class to compute probability for.\n",
    "- `word_counts (Dict[str, Dict[str, int]])`: Word counts per class.\n",
    "- `class_total_words (Dict[str, int])`: Total words per class.\n",
    "- `vocab_size (int)`: Size of the vocabulary.\n",
    "\n",
    "Returns:\n",
    "- `log_prob (float)`: The log probability of the word given the class.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> word_counts = {'a': {'hello': 5, 'world': 3}, 'b': {'hello': 2}}\n",
    ">>> class_totals = {'a': 8, 'b': 2}\n",
    ">>> vocab_size = 3\n",
    ">>> compute_word_log_likelihood('hello', 'a', word_counts, class_totals, vocab_size)\n",
    "-0.6061...  # log((5+1)/(8+3))\n",
    "```\n",
    "\n",
    "**Note:** Use `math.log` for the logarithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f8a58c",
   "metadata": {
    "id": "36f8a58c"
   },
   "outputs": [],
   "source": [
    "def compute_word_log_likelihood(word: str, class_label: str, word_counts: Dict[str, Dict[str, int]],\n",
    "                                  class_total_words: Dict[str, int], vocab_size: int) -> float:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    # word count for class (0 if word not seen)\n",
    "    word_count = word_counts.get(class_label, {}).get(word, 0)\n",
    "    # total words for class\n",
    "    total_words_in_class = class_total_words.get(class_label, 0)\n",
    "    \n",
    "    # Laplace smoothing: P(word|class) = (count + 1) / (total_words + vocab_size)\n",
    "    prob = (word_count + 1) / (total_words_in_class + vocab_size)\n",
    "    \n",
    "    log_prob = math.log(prob)\n",
    "    ### END CODE HERE\n",
    "    return log_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1402e455",
   "metadata": {
    "id": "1402e455"
   },
   "source": [
    "#### 5.3.4: Train the Classifier [ungraded]\n",
    "\n",
    "Run the cell below to train the Naive Bayes classifier on the training data. This computes all the parameters needed for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60cea97",
   "metadata": {
    "id": "a60cea97"
   },
   "outputs": [],
   "source": [
    "### DO NOT MODIFY THIS CELL ###\n",
    "\n",
    "# Compute prior probabilities\n",
    "priors = compute_priors(train_labels)\n",
    "print(\"Prior probabilities:\")\n",
    "for cls, prob in sorted(priors.items()):\n",
    "    print(f\"  P({cls}) = {prob:.4f}\")\n",
    "\n",
    "# Compute word counts and vocabulary\n",
    "word_counts, class_total_words, vocabulary = compute_word_counts(train_texts, train_labels)\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "print(f\"\\nVocabulary size: {vocab_size} unique words\")\n",
    "print(\"\\nTotal words per class:\")\n",
    "for cls, count in sorted(class_total_words.items()):\n",
    "    print(f\"  {cls}: {count} words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a08e45c",
   "metadata": {
    "id": "9a08e45c"
   },
   "source": [
    "### 5.4: Classify New Documents (5 points)\n",
    "\n",
    "Now implement the prediction function that uses the trained model to classify new documents.\n",
    "\n",
    "#### 5.4.1: Predict Class for a Single Document (5 points)\n",
    "\n",
    "Implement a function that predicts the most likely class for a given document. For each class, compute:\n",
    "$$\\log P(class|document) = \\log P(class) + \\sum_{word \\in document} \\log P(word|class)$$\n",
    "\n",
    "Return the class with the highest log probability.\n",
    "\n",
    "Arguments:\n",
    "- `text (str)`: The document to classify.\n",
    "- `priors (Dict[str, float])`: Prior probabilities for each class.\n",
    "- `word_counts (Dict[str, Dict[str, int]])`: Word counts per class.\n",
    "- `class_total_words (Dict[str, int])`: Total words per class.\n",
    "- `vocab_size (int)`: Size of the vocabulary.\n",
    "\n",
    "Returns:\n",
    "- `predicted_class (str)`: The predicted class label.\n",
    "- `log_probs (Dict[str, float])`: Dictionary mapping each class to its log probability for this document.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> text = \"new smartphone app released for mobile devices\"\n",
    ">>> predicted, log_probs = predict_class(text, priors, word_counts, class_total_words, vocab_size)\n",
    ">>> predicted\n",
    "'tech'\n",
    "```\n",
    "\n",
    "**Note:** Use `math.log` for the logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc65c0b",
   "metadata": {
    "id": "fdc65c0b"
   },
   "outputs": [],
   "source": [
    "def predict_class(text: str, priors: Dict[str, float], word_counts: Dict[str, Dict[str, int]],\n",
    "                  class_total_words: Dict[str, int], vocab_size: int) -> Tuple[str, Dict[str, float]]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    # Preprocess the text\n",
    "    tokens = preprocess_text(text)\n",
    "    \n",
    "    log_probs = {}\n",
    "    # log P(class|document) = log P(class) + sum(log P(word|class))\n",
    "    for class_label in priors:\n",
    "        # log prior\n",
    "        log_prob = math.log(priors[class_label])\n",
    "        # log likelihood for each word\n",
    "        for word in tokens:\n",
    "            log_prob += compute_word_log_likelihood(word, class_label, word_counts, \n",
    "                                                   class_total_words, vocab_size)\n",
    "        log_probs[class_label] = log_prob\n",
    "    \n",
    "    # class with highest log probability\n",
    "    predicted_class = max(log_probs.items(), key=lambda x: x[1])[0]\n",
    "    ### END CODE HERE\n",
    "    return predicted_class, log_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8528ce",
   "metadata": {
    "id": "4f8528ce"
   },
   "source": [
    "#### 5.4.2: Test on Sample Documents [ungraded]\n",
    "\n",
    "Run the cell below to test your classifier on some sample news headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6361898c",
   "metadata": {
    "id": "6361898c"
   },
   "outputs": [],
   "source": [
    "### DO NOT MODIFY THIS CELL ###\n",
    "# Test the classifier on sample documents\n",
    "sample_texts = [\n",
    "    \"New artificial intelligence model can write code and solve complex problems\",\n",
    "    \"Championship game goes into overtime as teams battle for victory\",\n",
    "    \"Stock prices surge as company reports record quarterly profits\",\n",
    "    \"Study finds exercise reduces risk of heart disease significantly\",\n",
    "    \"Senator proposes new bill to reform healthcare policy nationwide\"\n",
    "]\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\" * 70)\n",
    "for text in sample_texts:\n",
    "    predicted, log_probs = predict_class(text, priors, word_counts, class_total_words, vocab_size)\n",
    "    print(f\"\\nText: {text[:60]}...\")\n",
    "    print(f\"Predicted: {predicted}\")\n",
    "    print(\"Log probabilities:\", {k: f\"{v:.2f}\" for k, v in sorted(log_probs.items())})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5698491",
   "metadata": {
    "id": "f5698491"
   },
   "source": [
    "### 5.5: Evaluate the Classifier (3 points)\n",
    "\n",
    "Finally, let's evaluate how well our classifier performs on the test set.\n",
    "\n",
    "#### 5.5.1: Compute Accuracy (3 points)\n",
    "\n",
    "Implement a function that computes the classification accuracy on a test set.\n",
    "\n",
    "Arguments:\n",
    "- `test_texts (List[str])`: List of test documents.\n",
    "- `test_labels (List[str])`: True labels for test documents.\n",
    "- `priors (Dict[str, float])`: Prior probabilities.\n",
    "- `word_counts (Dict[str, Dict[str, int]])`: Word counts per class.\n",
    "- `class_total_words (Dict[str, int])`: Total words per class.\n",
    "- `vocab_size (int)`: Vocabulary size.\n",
    "\n",
    "Returns:\n",
    "- `accuracy (float)`: Proportion of correctly classified documents (between 0 and 1).\n",
    "- `predictions (List[str])`: List of predicted labels for each test document.\n",
    "\n",
    "Examples:\n",
    "```python\n",
    ">>> accuracy, predictions = compute_accuracy(test_texts, test_labels, priors, word_counts, class_total_words, vocab_size)\n",
    ">>> print(f\"Accuracy: {accuracy:.2%}\")\n",
    "Accuracy: 85.00%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019c580",
   "metadata": {
    "id": "a019c580"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(test_texts: List[str], test_labels: List[str], priors: Dict[str, float],\n",
    "                     word_counts: Dict[str, Dict[str, int]], class_total_words: Dict[str, int],\n",
    "                     vocab_size: int) -> Tuple[float, List[str]]:\n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    predictions = []\n",
    "    correct = 0\n",
    "    \n",
    "    # predict\n",
    "    for text in test_texts:\n",
    "        predicted, _ = predict_class(text, priors, word_counts, class_total_words, vocab_size)\n",
    "        predictions.append(predicted)\n",
    "    \n",
    "    # correct\n",
    "    for i in range(len(test_labels)):\n",
    "        if predictions[i] == test_labels[i]:\n",
    "            correct += 1\n",
    "    \n",
    "    # accuracy\n",
    "    accuracy = correct / len(test_labels) if len(test_labels) > 0 else 0.0\n",
    "    ### END CODE HERE\n",
    "    return accuracy, predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41474d",
   "metadata": {
    "id": "1f41474d"
   },
   "source": [
    "#### 5.5.2: Evaluate on Test Set [ungraded]\n",
    "\n",
    "Run the cell below to evaluate your Naive Bayes classifier on the test set and see detailed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a0694",
   "metadata": {
    "id": "829a0694"
   },
   "outputs": [],
   "source": [
    "### DO NOT MODIFY THIS CELL ###\n",
    "accuracy, predictions = compute_accuracy(test_texts, test_labels, priors, word_counts, class_total_words, vocab_size)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"NAIVE BAYES CLASSIFIER EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.2%} ({int(accuracy * len(test_labels))}/{len(test_labels)} correct)\")\n",
    "\n",
    "# Compute per-class accuracy\n",
    "print(\"\\nPer-class Results:\")\n",
    "classes = sorted(set(test_labels))\n",
    "for cls in classes:\n",
    "    # Get indices for this class\n",
    "    cls_indices = [i for i, label in enumerate(test_labels) if label == cls]\n",
    "    cls_correct = sum(1 for i in cls_indices if predictions[i] == cls)\n",
    "    cls_total = len(cls_indices)\n",
    "    cls_accuracy = cls_correct / cls_total if cls_total > 0 else 0\n",
    "    print(f\"  {cls}: {cls_accuracy:.2%} ({cls_correct}/{cls_total})\")\n",
    "\n",
    "# Show some misclassified examples\n",
    "print(\"\\nMisclassified Examples:\")\n",
    "misclassified = [(text, true, pred) for text, true, pred in zip(test_texts, test_labels, predictions) if true != pred]\n",
    "for text, true, pred in misclassified[:3]:\n",
    "    print(f\"  Text: {text[:50]}...\")\n",
    "    print(f\"  True: {true}, Predicted: {pred}\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
