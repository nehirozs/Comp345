{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "933af709"
      },
      "source": [
        "# COMP 345: Assignment 2 (100 points)\n",
        "\n",
        "This assignment will help you practice implementing classification evaluation metrics, linear regression from scratch, regularization techniques (Ridge and Lasso), and logistic regression for text classification. You'll work with the 7-book dataset to build and evaluate classification models using bag-of-words features.\n",
        "\n",
        "**Important Notes:**\n",
        "- Read all instructions carefully before writing code\n",
        "- Write code only between the marked regions   \n",
        "```python\n",
        "  ### WRITE YOUR CODE BELOW\n",
        "  # Your code here\n",
        "  ### END CODE HERE\n",
        "  ```\n",
        "- Do not modify cells marked with `### DO NOT MODIFY THIS CELL ###`\n",
        "- Do **not** change function signatures (function name, parameters, return type)\n",
        "- Make sure all functions return the specified types\n",
        "- Do **not** make in-cell imports. All packages are imported in the **first code cell**.\n",
        "- Test your code incrementally as you complete each function\n",
        "\n",
        "\n",
        "## How to Submit the Assignment?\n",
        "\n",
        "1. **Create a copy of the assignment**: Before starting, create a copy of this notebook in your Google Drive by clicking `File > Save a copy in Drive`. This ensures your progress is saved as you work.\n",
        "\n",
        "2. **Complete all exercises**: Work through each exercise in your copied notebook, writing your solutions between the designated code markers.\n",
        "\n",
        "3. **Download the notebook**: Once you have completed all exercises, download the notebook as an `.ipynb` file by clicking `File > Download > Download .ipynb` as shown below:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://drive.google.com/thumbnail?id=1SZc-bK8PzBmMsgI5iUY4g9AvKk128qO4&sz=w800\" alt=\"Download notebook from Colab\" width=\"800\">\n",
        "</p>\n",
        "\n",
        "4. **Rename the file**: Rename the downloaded file to `<student_id>_A2.ipynb` (e.g., `9284827_A2.ipynb`).\n",
        "\n",
        "5. **Submit on Gradescope**: Upload the renamed notebook file to Gradescope.\n",
        "\n",
        "## Questions?\n",
        "\n",
        "If you have any questions about the assignment, please reach out to the TA:\n",
        "- Slack: `#assignment-2`\n",
        "- Email: `jessica.ojo@mail.mcgill.ca` (**Note:** Please include `[COMP 345]` in the subject)\n",
        "- Office Hours: Mondays and Wednesdays, 1:45 pm ‚Äì 2:45 pm in McConnell Engineering Building Room 110 (Feb 9, 11, 16, 18)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIc9JJOJ4Hyf"
      },
      "source": [
        "# üö® REQUIRED: AI USAGE DISCLOSURE (READ CAREFULLY)\n",
        "\n",
        "**THIS SECTION IS MANDATORY FOR ALL STUDENTS**  \n",
        "\n",
        "Failure to complete this section **correctly and honestly** will result in a  \n",
        "### **‚Äì50% penalty on the assignment grade**  \n",
        "\n",
        "This applies **even if you did not use any AI tools**. Just fill in this section however you used AI tools, even if you did not use them at all!\n",
        "\n",
        "---\n",
        "\n",
        "## What to do\n",
        "\n",
        "If you used **any Generative AI tool** (e.g., ChatGPT, Claude, GitHub Copilot), you **must** declare:\n",
        "\n",
        "1. **Which AI tool(s)** you used  \n",
        "2. **Which exercise(s)** you used them for  \n",
        "3. **How** you used them (e.g., debugging, understanding code, small snippets)\n",
        "\n",
        "If you **did NOT** use any AI tools, you must **explicitly say so**.\n",
        "\n",
        "‚û°Ô∏è Leaving this section blank is treated as **non-disclosure**.\n",
        "\n",
        "We don't have any *exact* requirements about how you word this, and you don't need to be extremely specific. But you do need to give us a general sense of which tools you used, which questions you used them for, and the general things you used them to do.\n",
        "\n",
        "See the full AI usage policy here:  \n",
        "https://mcgill-nlp.github.io/teaching/comp345-ling345-W26/#generative-ai-policy\n",
        "\n",
        "**Note that we're actually quite liberal about how you use AI tools to help you, as long as you're not using them to completely replace your own work, and as long as you're honest!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gbZDLcZ_kHJ"
      },
      "source": [
        "## üëâüèªüëâüèªüëâüèª <font color=\"red\"> AI Usage Disclosure: </font> üëàüèªüëàüèªüëàüèª\n",
        "\n",
        "- AI tools used: Claude Code\n",
        "\n",
        "- These tools were used for the following questions: Each question\n",
        "\n",
        "- Ai tools were used to: Check my work, improve efficiency when necessary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byx1so7BAGIb"
      },
      "source": [
        "## Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqT1_ZMmoLzU"
      },
      "outputs": [],
      "source": [
        "### DO NOT MODIFY THIS CELL ###\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwG_LWQKoLzU"
      },
      "source": [
        "## 1. Classification Evaluation Metrics (15 points)\n",
        "\n",
        "This section tests your understanding of classification evaluation metrics. You'll implement functions to compute confusion matrices, precision, recall, F1-score, and other metrics from scratch. These metrics are essential for evaluating classification models, especially when dealing with imbalanced datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knjpX7Z-oLzU"
      },
      "source": [
        "### 1.1: Binary Confusion Matrix (4 points)\n",
        "\n",
        "A confusion matrix is a table that summarizes the performance of a classification model. For binary classification, it's a 2√ó2 matrix with the following structure:\n",
        "\n",
        "```\n",
        "                Predicted Negative    Predicted Positive\n",
        "Actual Negative        TN                    FP\n",
        "Actual Positive        FN                    TP\n",
        "```\n",
        "\n",
        "Where:\n",
        "- **True Positives (TP)**: Correctly predicted positive instances\n",
        "- **True Negatives (TN)**: Correctly predicted negative instances\n",
        "- **False Positives (FP)**: Negative instances incorrectly predicted as positive (Type I error)\n",
        "- **False Negatives (FN)**: Positive instances incorrectly predicted as negative (Type II error)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5ZFvKkroLzV"
      },
      "source": [
        "#### 1.1.1: Compute Confusion Matrix Components (2 points)\n",
        "\n",
        "This function computes the four components of a binary confusion matrix: TP, TN, FP, and FN.\n",
        "\n",
        "Arguments:\n",
        "- `y_true (List[int])`: True labels (0 or 1)\n",
        "- `y_pred (List[int])`: Predicted labels (0 or 1)\n",
        "- `positive_label (int)`: Which label to treat as positive (default: 1)\n",
        "\n",
        "Returns:\n",
        "- `(int, int, int, int)`: A tuple containing (TP, TN, FP, FN)\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> y_true = [1, 0, 1, 1, 0, 1, 0, 0]\n",
        ">>> y_pred = [1, 0, 1, 0, 0, 1, 1, 1]\n",
        ">>> compute_confusion_matrix_components(y_true, y_pred)\n",
        "(3, 2, 2, 1)  # TP=3, TN=2, FP=2, FN=1\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLvz-Ve-oLzV"
      },
      "outputs": [],
      "source": [
        "def compute_confusion_matrix_components(y_true: List[int], y_pred: List[int],\n",
        "                                         positive_label: int = 1) -> Tuple[int, int, int, int]:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    TP = sum(1 for i in range(len(y_true)) if y_true[i] == positive_label and y_pred[i] == positive_label)\n",
        "    TN = sum(1 for i in range(len(y_true)) if y_true[i] != positive_label and y_pred[i] != positive_label)\n",
        "    FP = sum(1 for i in range(len(y_true)) if y_true[i] != positive_label and y_pred[i] == positive_label)\n",
        "    FN = sum(1 for i in range(len(y_true)) if y_true[i] == positive_label and y_pred[i] != positive_label)\n",
        "    ### END CODE HERE\n",
        "    return TP, TN, FP, FN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0erosQooLzV"
      },
      "source": [
        "#### 1.1.2: Compute Accuracy from Confusion Matrix (1 points)\n",
        "\n",
        "Accuracy is the proportion of correct predictions:\n",
        "\n",
        "$$\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}$$\n",
        "\n",
        "Arguments:\n",
        "- `TP (int)`: True positives\n",
        "- `TN (int)`: True negatives\n",
        "- `FP (int)`: False positives\n",
        "- `FN (int)`: False negatives\n",
        "\n",
        "Returns:\n",
        "- `accuracy (float)`: The accuracy value (between 0 and 1)\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> compute_accuracy_from_cm(2, 3, 2, 1)\n",
        "0.625  # (2+3)/(2+3+2+1) = 5/8\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_wok6-7oLzV"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy_from_cm(TP: int, TN: int, FP: int, FN: int) -> float:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    total = TP + TN + FP + FN\n",
        "    accuracy = (TP + TN) / total if total > 0 else 0.0\n",
        "    ### END CODE HERE\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2Y-jssOoLzW"
      },
      "source": [
        "#### 1.1.3: Compute Precision (1 points)\n",
        "\n",
        "Precision answers: *\"Of all the instances we predicted as positive, how many were actually positive?\"*\n",
        "\n",
        "$$\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$$\n",
        "\n",
        "Arguments:\n",
        "- `TP (int)`: True positives\n",
        "- `FP (int)`: False positives\n",
        "\n",
        "Returns:\n",
        "- `precision (float)`: The precision value. Return 0.0 if TP + FP = 0.\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> compute_precision(2, 2)\n",
        "0.5  # 2/(2+2)\n",
        ">>> compute_precision(0, 0)\n",
        "0.0  # No positive predictions\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQjb8FnKoLzW"
      },
      "outputs": [],
      "source": [
        "def compute_precision(TP: int, FP: int) -> float:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
        "    ### END CODE HERE\n",
        "    return precision\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxC0ANB8oLzW"
      },
      "source": [
        "### 1.2: Additional Metrics (4 points)\n",
        "\n",
        "Now we'll implement recall and F1-score, which together with precision give us a comprehensive view of classification performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JB4lQKdoLzW"
      },
      "source": [
        "#### 1.2.1: Compute Recall (1 points)\n",
        "\n",
        "Recall (also called sensitivity or true positive rate) answers: *\"Of all the actual positive instances, how many did we correctly identify?\"*\n",
        "\n",
        "$$\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$$\n",
        "\n",
        "Arguments:\n",
        "- `TP (int)`: True positives\n",
        "- `FN (int)`: False negatives\n",
        "\n",
        "Returns:\n",
        "- `recall (float)`: The recall value. Return 0.0 if TP + FN = 0.\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> compute_recall(2, 1)\n",
        "0.6666666666666666  # 2/(2+1)\n",
        ">>> compute_recall(0, 0)\n",
        "0.0  # No positive instances\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-5aTxngoLzW"
      },
      "outputs": [],
      "source": [
        "def compute_recall(TP: int, FN: int) -> float:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
        "    ### END CODE HERE\n",
        "    return recall\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYYEuF5ZoLzW"
      },
      "source": [
        "#### 1.2.2: Compute F1-Score (2 points)\n",
        "\n",
        "F1-Score is the harmonic mean of precision and recall, providing a single metric that balances both:\n",
        "\n",
        "$$\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
        "\n",
        "The harmonic mean gives more weight to low values, so F1 will be low if either precision or recall is low.\n",
        "\n",
        "Arguments:\n",
        "- `precision (float)`: Precision value\n",
        "- `recall (float)`: Recall value\n",
        "\n",
        "Returns:\n",
        "- `f1_score (float)`: The F1-score. Return 0.0 if precision + recall = 0.\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> compute_f1_score(0.5, 0.666666)\n",
        "0.5714  # Approximately\n",
        ">>> compute_f1_score(1.0, 0.0)\n",
        "0.0  # If either is 0, F1 is 0\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-QLxTmFoLzW"
      },
      "outputs": [],
      "source": [
        "def compute_f1_score(precision: float, recall: float) -> float:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "    ### END CODE HERE\n",
        "    return f1_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqEzJNI8oLzW"
      },
      "source": [
        "#### 1.2.3: Compute All Binary Metrics (1 points)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This function combines all the previous functions to compute all binary classification metrics at once.\n",
        "\n",
        "Arguments:\n",
        "- `y_true (List[int])`: True labels\n",
        "- `y_pred (List[int])`: Predicted labels\n",
        "- `positive_label (int)`: Which label to treat as positive (default: 1)\n",
        "\n",
        "Returns:\n",
        "- `Dict[str, float]`: A dictionary with keys: 'accuracy', 'precision', 'recall', 'f1_score'\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> y_true = [1, 0, 1, 1, 0, 1, 0, 0]\n",
        ">>> y_pred = [1, 0, 1, 0, 0, 1, 1, 1]\n",
        ">>> metrics = compute_all_binary_metrics(y_true, y_pred)\n",
        ">>> metrics['accuracy']\n",
        "0.625\n",
        "```\n",
        "\n",
        "**Hint:** Use the functions you've already implemented!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TgirvAooLzX"
      },
      "outputs": [],
      "source": [
        "def compute_all_binary_metrics(y_true: List[int], y_pred: List[int],\n",
        "                               positive_label: int = 1) -> Dict[str, float]:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    TP, TN, FP, FN = compute_confusion_matrix_components(y_true, y_pred, positive_label)\n",
        "    accuracy = compute_accuracy_from_cm(TP, TN, FP, FN)\n",
        "    precision = compute_precision(TP, FP)\n",
        "    recall = compute_recall(TP, FN)\n",
        "    f1_score = compute_f1_score(precision, recall)\n",
        "    metrics = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1_score': f1_score}\n",
        "    ### END CODE HERE\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B9yLT4moLzX"
      },
      "source": [
        "### 1.3: Multi-Class Confusion Matrix (7 points)\n",
        "\n",
        "For multi-class classification, the confusion matrix generalizes to an n√ón matrix where n is the number of classes. Each row represents the true class, and each column represents the predicted class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUqQxxVjoLzX"
      },
      "source": [
        "#### 1.3.1: Build Confusion Matrix (3 points)\n",
        "\n",
        "This function creates a multi-class confusion matrix as a nested dictionary.\n",
        "\n",
        "Arguments:\n",
        "- `y_true (List[str])`: True class labels\n",
        "- `y_pred (List[str])`: Predicted class labels\n",
        "- `labels (List[str])`: List of all possible class labels (in order)\n",
        "\n",
        "Returns:\n",
        "- `Dict[str, Dict[str, int]]`: Confusion matrix where `cm[true_label][pred_label]` is the count\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> y_true = ['A', 'B', 'A', 'C', 'B', 'C']\n",
        ">>> y_pred = ['A', 'B', 'C', 'C', 'A', 'C']\n",
        ">>> labels = ['A', 'B', 'C']\n",
        ">>> cm = build_confusion_matrix(y_true, y_pred, labels)\n",
        ">>> cm['A']['A']  # True A, Predicted A\n",
        "1\n",
        ">>> cm['B']['A']  # True B, Predicted A\n",
        "1\n",
        ">>> cm['B']['C']  # True B, Predicted C\n",
        "0\n",
        "```\n",
        "\n",
        "**Note:** Initialize all cells to 0, then count the predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNxy8XDooLzX"
      },
      "outputs": [],
      "source": [
        "def build_confusion_matrix(y_true: List[str], y_pred: List[str],\n",
        "                           labels: List[str]) -> Dict[str, Dict[str, int]]:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    confusion_matrix = {true_lbl: {pred_lbl: 0 for pred_lbl in labels} for true_lbl in labels}\n",
        "    for i in range(len(y_true)):\n",
        "        confusion_matrix[y_true[i]][y_pred[i]] += 1\n",
        "    ### END CODE HERE\n",
        "    return confusion_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGZDdAjOoLzX"
      },
      "source": [
        "#### 1.3.2: Compute Per-Class Precision (2 points)\n",
        "\n",
        "For multi-class classification, we can compute precision for each class individually by treating it as a one-vs-all binary classification problem.\n",
        "\n",
        "For a specific class:\n",
        "$$\\text{Precision}_{\\text{class}} = \\frac{\\text{Correct predictions for class}}{\\text{Total predictions for class}}$$\n",
        "\n",
        "Arguments:\n",
        "- `confusion_matrix (Dict[str, Dict[str, int]])`: The confusion matrix from build_confusion_matrix\n",
        "- `labels (List[str])`: List of all class labels\n",
        "\n",
        "Returns:\n",
        "- `Dict[str, float]`: Dictionary mapping each class to its precision. Return 0.0 if no predictions for that class.\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> cm = {'A': {'A': 2, 'B': 1}, 'B': {'A': 0, 'B': 3}}\n",
        ">>> labels = ['A', 'B']\n",
        ">>> compute_per_class_precision(cm, labels)\n",
        "{'A': 1.0, 'B': 0.75}  # A: 2/2, B: 3/4\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7I0DpqGoLzX"
      },
      "outputs": [],
      "source": [
        "def compute_per_class_precision(confusion_matrix: Dict[str, Dict[str, int]],\n",
        "                                labels: List[str]) -> Dict[str, float]:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    precision_dict = {}\n",
        "    for cls in labels:\n",
        "        pred_for_cls = sum(confusion_matrix[true_lbl][cls] for true_lbl in labels)\n",
        "        correct = confusion_matrix[cls][cls]\n",
        "        precision_dict[cls] = correct / pred_for_cls if pred_for_cls > 0 else 0.0\n",
        "    ### END CODE HERE\n",
        "    return precision_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGcLlw0NoLzX"
      },
      "source": [
        "#### 1.3.3: Compute Per-Class Recall (2 points)\n",
        "\n",
        "Similarly, recall can be computed for each class:\n",
        "\n",
        "$$\\text{Recall}_{\\text{class}} = \\frac{\\text{Correct predictions for class}}{\\text{Total actual instances of class}}$$\n",
        "\n",
        "Arguments:\n",
        "- `confusion_matrix (Dict[str, Dict[str, int]])`: The confusion matrix\n",
        "- `labels (List[str])`: List of all class labels\n",
        "\n",
        "Returns:\n",
        "- `Dict[str, float]`: Dictionary mapping each class to its recall. Return 0.0 if no instances of that class.\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> cm = {'A': {'A': 2, 'B': 1}, 'B': {'A': 0, 'B': 3}}\n",
        ">>> labels = ['A', 'B']\n",
        ">>> compute_per_class_recall(cm, labels)\n",
        "{'A': 0.6666666666666666, 'B': 1.0}  # A: 2/3, B: 3/3\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhACJ3OboLzX"
      },
      "outputs": [],
      "source": [
        "def compute_per_class_recall(confusion_matrix: Dict[str, Dict[str, int]],\n",
        "                             labels: List[str]) -> Dict[str, float]:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    recall_dict = {}\n",
        "    for cls in labels:\n",
        "        actual_cls = sum(confusion_matrix[cls][pred_lbl] for pred_lbl in labels)\n",
        "        correct = confusion_matrix[cls][cls]\n",
        "        recall_dict[cls] = correct / actual_cls if actual_cls > 0 else 0.0\n",
        "    ### END CODE HERE\n",
        "    return recall_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW0tH8bOoLzY"
      },
      "source": [
        "## 2. Linear Regression from Scratch (15 points)\n",
        "\n",
        "In this section, you'll implement simple linear regression manually to understand how the model learns the relationship between features and targets. You'll compute predictions, residuals, and the R¬≤ metric.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lssj4CoioLzY"
      },
      "source": [
        "### 2.1: Making Predictions (6 points)\n",
        "\n",
        "Linear regression models the relationship as: $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n$\n",
        "\n",
        "Where:\n",
        "- $y$ is the predicted value\n",
        "- $\\beta_0$ is the intercept\n",
        "- $\\beta_1, \\beta_2, ..., \\beta_n$ are the coefficients for features $x_1, x_2, ..., x_n$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b7_0bproLzY"
      },
      "source": [
        "#### 2.1.1: Compute Single Prediction (2 points)\n",
        "\n",
        "This function computes a prediction for a single data point using the linear regression formula.\n",
        "\n",
        "Arguments:\n",
        "- `feature_values (List[float])`: Feature values for one instance (e.g., [2.5, 3.0])\n",
        "- `coefficients (List[float])`: Model coefficients (e.g., [1.5, 2.0])\n",
        "- `intercept (float)`: Model intercept\n",
        "\n",
        "Returns:\n",
        "- `prediction (float)`: The predicted value\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> compute_single_prediction([2.0, 3.0], [1.5, 2.0], 0.5)\n",
        "9.5  # 0.5 + 1.5*2.0 + 2.0*3.0 = 0.5 + 3.0 + 6.0\n",
        "```\n",
        "\n",
        "**Formula:** prediction = intercept + sum(coefficient[i] * feature_value[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pm3NdCRxoLzY"
      },
      "outputs": [],
      "source": [
        "def compute_single_prediction(feature_values: List[float], coefficients: List[float],\n",
        "                              intercept: float) -> float:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    prediction = intercept + sum(c * x for c, x in zip(coefficients, feature_values))\n",
        "    ### END CODE HERE\n",
        "    return prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVm6vWKXoLzY"
      },
      "source": [
        "#### 2.1.2: Compute All Predictions (2 points)\n",
        "\n",
        "This function computes predictions for multiple data points.\n",
        "\n",
        "Arguments:\n",
        "- `X (List[List[float]])`: Feature matrix where each inner list is one instance\n",
        "- `coefficients (List[float])`: Model coefficients\n",
        "- `intercept (float)`: Model intercept\n",
        "\n",
        "Returns:\n",
        "- `predictions (List[float])`: List of predictions, one for each instance\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> X = [[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]]\n",
        ">>> coefficients = [1.0, 1.0]\n",
        ">>> intercept = 0.5\n",
        ">>> compute_predictions(X, coefficients, intercept)\n",
        "[3.5, 5.5, 7.5]\n",
        "```\n",
        "\n",
        "**Hint:** Use the `compute_single_prediction` function for each row in X.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAJF-DIIoLzY"
      },
      "outputs": [],
      "source": [
        "def compute_predictions(X: List[List[float]], coefficients: List[float],\n",
        "                        intercept: float) -> List[float]:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    predictions = [compute_single_prediction(row, coefficients, intercept) for row in X]\n",
        "    ### END CODE HERE\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtdy13kXoLzY"
      },
      "source": [
        "#### 2.1.3: Compute Residuals (1 points)\n",
        "\n",
        "Residuals are the differences between actual and predicted values: $\\text{residual} = y_{\\text{actual}} - y_{\\text{predicted}}$\n",
        "\n",
        "Arguments:\n",
        "- `y_true (List[float])`: Actual target values\n",
        "- `y_pred (List[float])`: Predicted target values\n",
        "\n",
        "Returns:\n",
        "- `residuals (List[float])`: List of residuals\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> y_true = [3.0, 5.0, 7.0]\n",
        ">>> y_pred = [3.5, 5.5, 6.5]\n",
        ">>> compute_residuals(y_true, y_pred)\n",
        "[-0.5, -0.5, 0.5]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Egad7qBpoLzY"
      },
      "outputs": [],
      "source": [
        "def compute_residuals(y_true: List[float], y_pred: List[float]) -> List[float]:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    residuals = [y_true[i] - y_pred[i] for i in range(len(y_true))]\n",
        "    ### END CODE HERE\n",
        "    return residuals\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UmV_fUVoLzY"
      },
      "source": [
        "#### 2.1.4: Compute Mean Squared Error (1 points)\n",
        "\n",
        "Mean Squared Error (MSE) measures the average squared difference between predictions and actual values:\n",
        "\n",
        "$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "Arguments:\n",
        "- `y_true (List[float])`: Actual values\n",
        "- `y_pred (List[float])`: Predicted values\n",
        "\n",
        "Returns:\n",
        "- `mse (float)`: The mean squared error\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> y_true = [3.0, 5.0, 7.0]\n",
        ">>> y_pred = [3.5, 5.5, 6.5]\n",
        ">>> compute_mse(y_true, y_pred)\n",
        "0.25  # ((0.5)^2 + (0.5)^2 + (0.5)^2) / 3\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4O1-tEMnoLzY"
      },
      "outputs": [],
      "source": [
        "def compute_mse(y_true: List[float], y_pred: List[float]) -> float:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    n = len(y_true)\n",
        "    mse = sum((y_true[i] - y_pred[i]) ** 2 for i in range(n)) / n if n > 0 else 0.0\n",
        "    ### END CODE HERE\n",
        "    return mse\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz9kHolCoLzY"
      },
      "source": [
        "### 2.2: R-Squared Metric (9 points)\n",
        "\n",
        "R¬≤ (coefficient of determination) measures how well the model explains the variance in the data. It ranges from 0 to 1 (though it can be negative for very poor models), where 1 means perfect predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgbCXL4PoLzY"
      },
      "source": [
        "#### 2.2.1: Compute Sum of Squared Residuals (2 points)\n",
        "\n",
        "This is the sum of squared differences between actual and predicted values:\n",
        "\n",
        "$$SS_{\\text{res}} = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "Arguments:\n",
        "- `y_true (List[float])`: Actual values\n",
        "- `y_pred (List[float])`: Predicted values\n",
        "\n",
        "Returns:\n",
        "- `ss_res (float)`: Sum of squared residuals\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> y_true = [3.0, 5.0, 7.0]\n",
        ">>> y_pred = [3.5, 5.5, 6.5]\n",
        ">>> compute_ss_res(y_true, y_pred)\n",
        "0.75  # (0.5)^2 + (0.5)^2 + (0.5)^2\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAXACKBdoLzY"
      },
      "outputs": [],
      "source": [
        "def compute_ss_res(y_true: List[float], y_pred: List[float]) -> float:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    ss_res = sum((y_true[i] - y_pred[i]) ** 2 for i in range(len(y_true)))\n",
        "    ### END CODE HERE\n",
        "    return ss_res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwaAGIDYoLzZ"
      },
      "source": [
        "#### 2.2.2: Compute Total Sum of Squares (2 points)\n",
        "\n",
        "This is the sum of squared differences between actual values and their mean:\n",
        "\n",
        "$$SS_{\\text{tot}} = \\sum_{i=1}^{n}(y_i - \\bar{y})^2$$\n",
        "\n",
        "where $\\bar{y}$ is the mean of $y$.\n",
        "\n",
        "Arguments:\n",
        "- `y_true (List[float])`: Actual values\n",
        "\n",
        "Returns:\n",
        "- `ss_tot (float)`: Total sum of squares\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> y_true = [3.0, 5.0, 7.0]\n",
        ">>> compute_ss_tot(y_true)\n",
        "8.0  # mean=5.0, (3-5)^2 + (5-5)^2 + (7-5)^2 = 4 + 0 + 4\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cLLCShioLzZ"
      },
      "outputs": [],
      "source": [
        "def compute_ss_tot(y_true: List[float]) -> float:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    mean_y = sum(y_true) / len(y_true) if len(y_true) > 0 else 0.0\n",
        "    ss_tot = sum((y - mean_y) ** 2 for y in y_true)\n",
        "    ### END CODE HERE\n",
        "    return ss_tot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KjJMD9RoLzZ"
      },
      "source": [
        "#### 2.2.3: Compute R-Squared (2 points)\n",
        "\n",
        "R¬≤ is calculated as:\n",
        "\n",
        "$$R^2 = 1 - \\frac{SS_{\\text{res}}}{SS_{\\text{tot}}}$$\n",
        "\n",
        "Arguments:\n",
        "- `ss_res (float)`: Sum of squared residuals\n",
        "- `ss_tot (float)`: Total sum of squares\n",
        "\n",
        "Returns:\n",
        "- `r_squared (float)`: The R¬≤ value. Return 0.0 if ss_tot is 0.\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> compute_r_squared(0.75, 8.0)\n",
        "0.90625  # 1 - 0.75/8.0\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDXnyHNpoLzZ"
      },
      "outputs": [],
      "source": [
        "def compute_r_squared(ss_res: float, ss_tot: float) -> float:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    r_squared = 1.0 - (ss_res / ss_tot) if ss_tot > 0 else 0.0\n",
        "    ### END CODE HERE\n",
        "    return r_squared\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzEHNu5GoLzZ"
      },
      "source": [
        "#### 2.2.4: Compute Adjusted R-Squared (2 points)\n",
        "\n",
        "Adjusted R¬≤ accounts for the number of features in the model:\n",
        "\n",
        "$$R_{\\text{adj}}^2 = 1 - \\frac{(1-R^2)(n-1)}{n-p-1}$$\n",
        "\n",
        "where:\n",
        "- $n$ is the number of samples\n",
        "- $p$ is the number of features\n",
        "\n",
        "Arguments:\n",
        "- `r_squared (float)`: Regular R¬≤ value\n",
        "- `n_samples (int)`: Number of data points\n",
        "- `n_features (int)`: Number of features\n",
        "\n",
        "Returns:\n",
        "- `adj_r_squared (float)`: The adjusted R¬≤ value\n",
        "\n",
        "Examples:\n",
        "Examples:\n",
        "```python\n",
        ">>> compute_adjusted_r_squared(0.90625, 3, 2)\n",
        "0.90625  \n",
        ">>> compute_adjusted_r_squared(0.90625, 4, 2)\n",
        "0.71875  \n",
        "```\n",
        "\n",
        "**Note:** If n <= p + 1, return the regular r_squared value to avoid division issues.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFHfllDkoLza"
      },
      "outputs": [],
      "source": [
        "def compute_adjusted_r_squared(r_squared: float, n_samples: int, n_features: int) -> float:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    if n_samples <= n_features + 1:\n",
        "        adj_r_squared = r_squared\n",
        "    else:\n",
        "        adj_r_squared = 1.0 - (1.0 - r_squared) * (n_samples - 1) / (n_samples - n_features - 1)\n",
        "    ### END CODE HERE\n",
        "    return adj_r_squared\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmmUBXe3oLza"
      },
      "source": [
        "#### 2.2.5: Compute All Regression Metrics (1 points)\n",
        "\n",
        "Combine all the metrics into a single function.\n",
        "\n",
        "Arguments:\n",
        "- `y_true (List[float])`: Actual values\n",
        "- `y_pred (List[float])`: Predicted values\n",
        "- `n_features (int)`: Number of features used in the model\n",
        "\n",
        "Returns:\n",
        "- `Dict[str, float]`: Dictionary with keys: 'mse', 'r_squared', 'adjusted_r_squared'\n",
        "\n",
        "**Hint:** Use the functions you've already implemented!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeTFi5VdoLza"
      },
      "outputs": [],
      "source": [
        "def compute_all_regression_metrics(y_true: List[float], y_pred: List[float],\n",
        "                                   n_features: int) -> Dict[str, float]:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    mse = compute_mse(y_true, y_pred)\n",
        "    ss_res = compute_ss_res(y_true, y_pred)\n",
        "    ss_tot = compute_ss_tot(y_true)\n",
        "    r_squared = compute_r_squared(ss_res, ss_tot)\n",
        "    n_samples = len(y_true)\n",
        "    adjusted_r_squared = compute_adjusted_r_squared(r_squared, n_samples, n_features)\n",
        "    metrics = {'mse': mse, 'r_squared': r_squared, 'adjusted_r_squared': adjusted_r_squared}\n",
        "    ### END CODE HERE\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toLmPadVoLza"
      },
      "source": [
        "## 3. Regularization for Regression (15 points)\n",
        "\n",
        "Regularization helps prevent overfitting by adding a penalty term to the loss function. You'll implement Ridge (L2) and Lasso (L1) regularization penalties and understand how they affect model coefficients.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQbG_d-poLza"
      },
      "source": [
        "### 3.1: Ridge Regression (L2 Regularization) (6 points)\n",
        "\n",
        "Ridge regression adds an L2 penalty term proportional to the sum of squared coefficients:\n",
        "\n",
        "$$\\text{Loss}_{\\text{Ridge}} = \\text{MSE} + \\lambda \\sum_{j=1}^{p}\\beta_j^2$$\n",
        "\n",
        "where $\\lambda$ is the regularization parameter (alpha in sklearn).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_funOdNoLza"
      },
      "source": [
        "#### 3.1.1: Compute L2 Penalty (3 points)\n",
        "\n",
        "The L2 penalty is the sum of squared coefficients multiplied by lambda:\n",
        "\n",
        "$$\\text{L2 Penalty} = \\lambda \\sum_{j=1}^{p}\\beta_j^2$$\n",
        "\n",
        "Arguments:\n",
        "- `coefficients (List[float])`: Model coefficients (not including intercept)\n",
        "- `lambda_param (float)`: Regularization parameter\n",
        "\n",
        "Returns:\n",
        "- `l2_penalty (float)`: The L2 penalty value\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> coefficients = [2.0, 3.0, 1.0]\n",
        ">>> lambda_param = 0.1\n",
        ">>> compute_l2_penalty(coefficients, lambda_param)\n",
        "1.4  # 0.1 * (2^2 + 3^2 + 1^2) = 0.1 * 14\n",
        "```\n",
        "\n",
        "**Note:** The intercept is never penalized in regularization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKI8KU60oLza"
      },
      "outputs": [],
      "source": [
        "def compute_l2_penalty(coefficients: List[float], lambda_param: float) -> float:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    l2_penalty = lambda_param * sum(c ** 2 for c in coefficients)\n",
        "    ### END CODE HERE\n",
        "    return l2_penalty\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pxA06ipoLza"
      },
      "source": [
        "#### 3.1.2: Compute Ridge Loss (3 points)\n",
        "\n",
        "Ridge loss combines MSE with the L2 penalty:\n",
        "\n",
        "$$\\text{Ridge Loss} = \\text{MSE} + \\text{L2 Penalty}$$\n",
        "\n",
        "Arguments:\n",
        "- `y_true (List[float])`: Actual values\n",
        "- `y_pred (List[float])`: Predicted values\n",
        "- `coefficients (List[float])`: Model coefficients\n",
        "- `lambda_param (float)`: Regularization parameter\n",
        "\n",
        "Returns:\n",
        "- `ridge_loss (float)`: Total Ridge loss\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> y_true = [3.0, 5.0, 7.0]\n",
        ">>> y_pred = [3.5, 5.5, 6.5]\n",
        ">>> coefficients = [1.0, 1.0]\n",
        ">>> lambda_param = 0.1\n",
        ">>> compute_ridge_loss(y_true, y_pred, coefficients, lambda_param)\n",
        "0.45  # MSE + L2_penalty\n",
        "```\n",
        "\n",
        "**Hint:** Use `compute_mse` and `compute_l2_penalty`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L2PKuR0oLza"
      },
      "outputs": [],
      "source": [
        "def compute_ridge_loss(y_true: List[float], y_pred: List[float],\n",
        "                       coefficients: List[float], lambda_param: float) -> float:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    mse = compute_mse(y_true, y_pred)\n",
        "    l2_penalty = compute_l2_penalty(coefficients, lambda_param)\n",
        "    ridge_loss = mse + l2_penalty\n",
        "    ### END CODE HERE\n",
        "    return ridge_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeWxG8LIoLzb"
      },
      "source": [
        "### 3.2: Lasso Regression (L1 Regularization) (9 points)\n",
        "\n",
        "Lasso regression adds an L1 penalty proportional to the sum of absolute values of coefficients:\n",
        "\n",
        "$$\\text{Loss}_{\\text{Lasso}} = \\text{MSE} + \\lambda \\sum_{j=1}^{p}|\\beta_j|$$\n",
        "\n",
        "L1 regularization encourages sparsity (some coefficients become exactly zero).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqpxTFDeoLzb"
      },
      "source": [
        "#### 3.2.1: Compute L1 Penalty (2 points)\n",
        "\n",
        "The L1 penalty is the sum of absolute values of coefficients:\n",
        "\n",
        "$$\\text{L1 Penalty} = \\lambda \\sum_{j=1}^{p}|\\beta_j|$$\n",
        "\n",
        "Arguments:\n",
        "- `coefficients (List[float])`: Model coefficients\n",
        "- `lambda_param (float)`: Regularization parameter\n",
        "\n",
        "Returns:\n",
        "- `l1_penalty (float)`: The L1 penalty value\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> coefficients = [2.0, -3.0, 1.0]\n",
        ">>> lambda_param = 0.1\n",
        ">>> compute_l1_penalty(coefficients, lambda_param)\n",
        "0.6  # 0.1 * (|2| + |-3| + |1|) = 0.1 * 6\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3NcVg5VoLzb"
      },
      "outputs": [],
      "source": [
        "def compute_l1_penalty(coefficients: List[float], lambda_param: float) -> float:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    l1_penalty = lambda_param * sum(abs(c) for c in coefficients)\n",
        "    ### END CODE HERE\n",
        "    return l1_penalty\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-t7g-troLzb"
      },
      "source": [
        "#### 3.2.2: Compute Lasso Loss (2 points)\n",
        "\n",
        "Lasso loss combines MSE with the L1 penalty.\n",
        "\n",
        "Arguments:\n",
        "- `y_true (List[float])`: Actual values\n",
        "- `y_pred (List[float])`: Predicted values\n",
        "- `coefficients (List[float])`: Model coefficients\n",
        "- `lambda_param (float)`: Regularization parameter\n",
        "\n",
        "Returns:\n",
        "- `lasso_loss (float)`: Total Lasso loss\n",
        "\n",
        "**Hint:** Use `compute_mse` and `compute_l1_penalty`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21UHWm2RoLzb"
      },
      "outputs": [],
      "source": [
        "def compute_lasso_loss(y_true: List[float], y_pred: List[float],\n",
        "                        coefficients: List[float], lambda_param: float) -> float:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    mse = compute_mse(y_true, y_pred)\n",
        "    l1_penalty = compute_l1_penalty(coefficients, lambda_param)\n",
        "    lasso_loss = mse + l1_penalty\n",
        "    ### END CODE HERE\n",
        "    return lasso_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd-c_T5poLzb"
      },
      "source": [
        "#### 3.2.3: Count Zero Coefficients (2 points)\n",
        "\n",
        "One key property of Lasso is that it can drive some coefficients to exactly zero, effectively performing feature selection.\n",
        "\n",
        "Arguments:\n",
        "- `coefficients (List[float])`: Model coefficients\n",
        "\n",
        "Returns:\n",
        "- `zero_count (int)`: Number of coefficients that are exactly 0.0\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> coefficients = [2.0, 0.0, 1.0, 0.0, 0.0]\n",
        ">>> count_zero_coefficients(coefficients)\n",
        "3\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vu-k6oFooLzb"
      },
      "outputs": [],
      "source": [
        "def count_zero_coefficients(coefficients: List[float]) -> int:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    zero_count = sum(1 for c in coefficients if c == 0.0)\n",
        "    ### END CODE HERE\n",
        "    return zero_count\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRctumYEoLzb"
      },
      "source": [
        "#### 3.2.4: Identify Selected Features (3 points)\n",
        "\n",
        "This function identifies which features are selected by Lasso (have non-zero coefficients).\n",
        "\n",
        "Arguments:\n",
        "- `coefficients (List[float])`: Model coefficients\n",
        "- `feature_names (List[str])`: Names of features corresponding to coefficients\n",
        "\n",
        "Returns:\n",
        "- `selected_features (List[str])`: Names of features with non-zero coefficients, in original order\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> coefficients = [2.0, 0.0, 1.5, 0.0]\n",
        ">>> feature_names = ['age', 'income', 'score', 'rating']\n",
        ">>> identify_selected_features(coefficients, feature_names)\n",
        "['age', 'score']\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcw5eqEIoLzb"
      },
      "outputs": [],
      "source": [
        "def identify_selected_features(coefficients: List[float],\n",
        "                               feature_names: List[str]) -> List[str]:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    selected_features = [feature_names[i] for i in range(len(coefficients)) if coefficients[i] != 0.0]\n",
        "    ### END CODE HERE\n",
        "    return selected_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKPSL2sHoLzb"
      },
      "source": [
        "## 4. Logistic Regression for Text Classification (15 points)\n",
        "\n",
        "In this section, you'll implement key components of logistic regression for binary text classification. Logistic regression uses the sigmoid function to convert linear combinations into probabilities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je50y4WtoLzb"
      },
      "source": [
        "### 4.1: Probability Calculations (8 points)\n",
        "\n",
        "Logistic regression models the probability of the positive class using the sigmoid function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuHGznLFoLzb"
      },
      "source": [
        "#### 4.1.1: Compute Log-Odds (2 points)\n",
        "\n",
        "Log-odds (logit) is the linear combination of features and coefficients:\n",
        "\n",
        "$$\\text{log-odds} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n$$\n",
        "\n",
        "This is identical to linear regression prediction, but we interpret it differently.\n",
        "\n",
        "Arguments:\n",
        "- `feature_values (List[float])`: Feature values for one instance\n",
        "- `coefficients (List[float])`: Model coefficients\n",
        "- `intercept (float)`: Model intercept\n",
        "\n",
        "Returns:\n",
        "- `log_odds (float)`: The log-odds value\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> feature_values = [2.0, 3.0]\n",
        ">>> coefficients = [0.5, 0.3]\n",
        ">>> intercept = -1.0\n",
        ">>> compute_log_odds(feature_values, coefficients, intercept)\n",
        "0.9  # -1.0 + 0.5*2.0 + 0.3*3.0\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-B5suZdXoLzb"
      },
      "outputs": [],
      "source": [
        "def compute_log_odds(feature_values: List[float], coefficients: List[float],\n",
        "                     intercept: float) -> float:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    log_odds = intercept + sum(c * x for c, x in zip(coefficients, feature_values))\n",
        "    ### END CODE HERE\n",
        "    return log_odds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eow4iywSoLzb"
      },
      "source": [
        "#### 4.1.2: Sigmoid Function (2 points)\n",
        "\n",
        "The sigmoid function converts log-odds to probability:\n",
        "\n",
        "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "This function always outputs a value between 0 and 1.\n",
        "\n",
        "Arguments:\n",
        "- `z (float)`: The log-odds value\n",
        "\n",
        "Returns:\n",
        "- `probability (float)`: Value between 0 and 1\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> sigmoid(0.0)\n",
        "0.5  # At 0, probability is 50%\n",
        ">>> sigmoid(2.0)\n",
        "0.8807970779778823  # Positive log-odds ‚Üí high probability\n",
        ">>> sigmoid(-2.0)\n",
        "0.11920292202211755  # Negative log-odds ‚Üí low probability\n",
        "```\n",
        "\n",
        "**Hint:** Use `math.exp()` for the exponential function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYDHJZOHoLzc"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z: float) -> float:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    probability = 1.0 / (1.0 + math.exp(-z))\n",
        "    ### END CODE HERE\n",
        "    return probability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxjkGLGYoLzc"
      },
      "source": [
        "#### 4.1.3: Predict Probability (2 points)\n",
        "\n",
        "Combine log-odds calculation and sigmoid to get the probability of the positive class for a single instance.\n",
        "\n",
        "Arguments:\n",
        "- `feature_values (List[float])`: Feature values for one instance\n",
        "- `coefficients (List[float])`: Model coefficients\n",
        "- `intercept (float)`: Model intercept\n",
        "\n",
        "Returns:\n",
        "- `probability (float)`: Probability of positive class (between 0 and 1)\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> feature_values = [2.0, 3.0]\n",
        ">>> coefficients = [0.5, 0.3]\n",
        ">>> intercept = -1.0\n",
        ">>> predict_probability(feature_values, coefficients, intercept)\n",
        "0.7109495026250039  # sigmoid(0.9)\n",
        "```\n",
        "\n",
        "**Hint:** Use `compute_log_odds` and `sigmoid`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amlEZQQNoLzc"
      },
      "outputs": [],
      "source": [
        "def predict_probability(feature_values: List[float], coefficients: List[float],\n",
        "                        intercept: float) -> float:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    log_odds = compute_log_odds(feature_values, coefficients, intercept)\n",
        "    probability = sigmoid(log_odds)\n",
        "    ### END CODE HERE\n",
        "    return probability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NIn_OFsoLzc"
      },
      "source": [
        "#### 4.1.4: Predict Probabilities for All Instances (2 points)\n",
        "\n",
        "Compute probabilities for multiple instances.\n",
        "\n",
        "Arguments:\n",
        "- `X (List[List[float]])`: Feature matrix\n",
        "- `coefficients (List[float])`: Model coefficients\n",
        "- `intercept (float)`: Model intercept\n",
        "\n",
        "Returns:\n",
        "- `probabilities (List[float])`: List of probabilities, one per instance\n",
        "\n",
        "**Hint:** Use `predict_probability` for each row.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rX1FgT2oLzc"
      },
      "outputs": [],
      "source": [
        "def predict_probabilities(X: List[List[float]], coefficients: List[float],\n",
        "                          intercept: float) -> List[float]:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    probabilities = [predict_probability(row, coefficients, intercept) for row in X]\n",
        "    ### END CODE HERE\n",
        "    return probabilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQKFaOAXoLzc"
      },
      "source": [
        "### 4.2: Classification Decisions (7 points)\n",
        "\n",
        "Once we have probabilities, we need to convert them to class predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7k1hHj9oLzc"
      },
      "source": [
        "#### 4.2.1: Make Binary Prediction (2 points)\n",
        "\n",
        "Convert a probability to a binary class prediction using a threshold.\n",
        "\n",
        "Arguments:\n",
        "- `probability (float)`: Predicted probability of positive class\n",
        "- `threshold (float)`: Decision threshold (default: 0.5)\n",
        "\n",
        "Returns:\n",
        "- `prediction (int)`: 1 if probability >= threshold, else 0\n",
        "\n",
        "Examples:\n",
        "```python\n",
        ">>> make_binary_prediction(0.7, 0.5)\n",
        "1\n",
        ">>> make_binary_prediction(0.3, 0.5)\n",
        "0\n",
        ">>> make_binary_prediction(0.7, 0.8)\n",
        "0\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_iuf8YhoLzc"
      },
      "outputs": [],
      "source": [
        "def make_binary_prediction(probability: float, threshold: float = 0.5) -> int:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    prediction = 1 if probability >= threshold else 0\n",
        "    ### END CODE HERE\n",
        "    return prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqSoBUWqoLzc"
      },
      "source": [
        "#### 4.2.2: Make All Predictions (2 points)\n",
        "\n",
        "Convert a list of probabilities to binary predictions.\n",
        "\n",
        "Arguments:\n",
        "- `probabilities (List[float])`: List of predicted probabilities\n",
        "- `threshold (float)`: Decision threshold (default: 0.5)\n",
        "\n",
        "Returns:\n",
        "- `predictions (List[int])`: List of binary predictions (0 or 1)\n",
        "\n",
        "**Hint:** Use `make_binary_prediction` for each probability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAflvdZ-oLzc"
      },
      "outputs": [],
      "source": [
        "def make_all_predictions(probabilities: List[float], threshold: float = 0.5) -> List[int]:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    predictions = [make_binary_prediction(p, threshold) for p in probabilities]\n",
        "    ### END CODE HERE\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArF2h-S-oLzc"
      },
      "source": [
        "#### 4.2.3: Full Prediction Pipeline (3 points)\n",
        "\n",
        "Combine all steps to go from features to binary predictions.\n",
        "\n",
        "Arguments:\n",
        "- `X (List[List[float]])`: Feature matrix\n",
        "- `coefficients (List[float])`: Model coefficients\n",
        "- `intercept (float)`: Model intercept\n",
        "- `threshold (float)`: Decision threshold (default: 0.5)\n",
        "\n",
        "Returns:\n",
        "- `predictions (List[int])`: Binary predictions for all instances\n",
        "\n",
        "**Hint:** Use `predict_probabilities` and `make_all_predictions`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNzEfbANoLzc"
      },
      "outputs": [],
      "source": [
        "def full_prediction_pipeline(X: List[List[float]], coefficients: List[float],\n",
        "                             intercept: float, threshold: float = 0.5) -> List[int]:\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "    probabilities = predict_probabilities(X, coefficients, intercept)\n",
        "    predictions = make_all_predictions(probabilities, threshold)\n",
        "    ### END CODE HERE\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axa1PqXooLzc"
      },
      "source": [
        "## 5. Building Your Text Classification Model (40 points)\n",
        "\n",
        "In this final section, you'll build a complete text classification pipeline using logistic regression to distinguish sentences from **Emma** by Jane Austen from sentences in the other 6 books. You'll make key modeling decisions about regularization and evaluate your model using the metrics you implemented earlier.\n",
        "\n",
        "**This section is worth 40 points** - the most heavily weighted section of the assignment. You'll be graded on:\n",
        "- Correct implementation of each step\n",
        "- Proper use of sklearn's LogisticRegression\n",
        "- Appropriate model selection based on performance\n",
        "- Correct output format\n",
        "\n",
        "**Important**: You must complete all previous sections first, as you'll use the functions you implemented.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPCqy_t4oLzd"
      },
      "outputs": [],
      "source": [
        "### DO NOT MODIFY THIS CELL ###\n",
        "\n",
        "# Load the preprocessed 7-book dataset\n",
        "train_7book = pd.read_csv('https://raw.githubusercontent.com/grvkamath/temp-for-nl2ds/refs/heads/main/classification_evaluation_data/7book_train.csv')\n",
        "test_7book = pd.read_csv('https://raw.githubusercontent.com/grvkamath/temp-for-nl2ds/refs/heads/main/classification_evaluation_data/7book_test.csv')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"7-BOOK DATASET LOADED\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Training set: {len(train_7book):,} sentences\")\n",
        "print(f\"Test set: {len(test_7book):,} sentences\")\n",
        "print(f\"\\nFeatures: {len(train_7book.columns) - 2} word features\")\n",
        "print(f\"\\nBooks in dataset:\")\n",
        "for book in sorted(train_7book['source'].unique()):\n",
        "    count = len(train_7book[train_7book['source'] == book])\n",
        "    print(f\"  - {book}: {count:,} sentences\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6k5biR4oLzd"
      },
      "source": [
        "### 5.1: Load and Prepare the Data (5 points)\n",
        "\n",
        "First, create the binary classification task and prepare your training and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zTJEgnqoLzd"
      },
      "outputs": [],
      "source": [
        "### DO NOT MODIFY THIS CELL ###\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"LOADING AND PREPARING DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create binary classification: Emma vs. All Other Books\n",
        "train_7book['is_emma'] = (train_7book['source'] == 'Emma').astype(int)\n",
        "test_7book['is_emma'] = (test_7book['source'] == 'Emma').astype(int)\n",
        "\n",
        "# Get feature columns (exclude metadata columns)\n",
        "feature_cols = [col for col in train_7book.columns\n",
        "                if col not in ['source', 'sentence', 'is_emma']]\n",
        "\n",
        "print(f\"\\nClassification Task: Emma vs. Other Books\")\n",
        "print(f\"  Training samples: {len(train_7book):,}\")\n",
        "print(f\"    - Emma: {train_7book['is_emma'].sum():,} ({train_7book['is_emma'].mean():.1%})\")\n",
        "print(f\"    - Other: {(1-train_7book['is_emma']).sum():,} ({(1-train_7book['is_emma']).mean():.1%})\")\n",
        "print(f\"  Test samples: {len(test_7book):,}\")\n",
        "print(f\"    - Emma: {test_7book['is_emma'].sum():,} ({test_7book['is_emma'].mean():.1%})\")\n",
        "print(f\"    - Other: {(1-test_7book['is_emma']).sum():,} ({(1-test_7book['is_emma']).mean():.1%})\")\n",
        "print(f\"\\nNumber of features: {len(feature_cols)}\")\n",
        "print(f\"\\n‚úì Data prepared!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee04kzOFoLzd"
      },
      "source": [
        "#### Task 5.1: Extract Features and Labels (5 points)\n",
        "\n",
        "Extract the feature matrices and label vectors for both training and test sets.\n",
        "\n",
        "**Requirements**:\n",
        "- Use the `feature_cols` defined above\n",
        "- Extract as numpy arrays using `.values`\n",
        "- Name variables exactly as specified below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGUltY7PoLzd"
      },
      "outputs": [],
      "source": [
        "# TODO: Extract training and test data\n",
        "# Create X_train, y_train, X_test, y_test\n",
        "\n",
        "### WRITE YOUR CODE BELOW\n",
        "\n",
        "# Extract training features and labels\n",
        "X_train = train_7book[feature_cols].values\n",
        "y_train = train_7book['is_emma'].values\n",
        "\n",
        "# Extract test features and labels\n",
        "X_test = test_7book[feature_cols].values\n",
        "y_test = test_7book['is_emma'].values\n",
        "\n",
        "### END CODE HERE\n",
        "\n",
        "# Verify shapes\n",
        "print(f\"\\nData shapes:\")\n",
        "print(f\"  X_train: {X_train.shape}\")\n",
        "print(f\"  y_train: {y_train.shape}\")\n",
        "print(f\"  X_test: {X_test.shape}\")\n",
        "print(f\"  y_test: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugk3RI8KoLzd"
      },
      "source": [
        "### 5.2: Train Models with Different Regularization (15 points)\n",
        "\n",
        "In class, we covered regularization (both Ridge and Lasso) in the context of linear regressions.\n",
        "But as we'll see in this question, the same regularization techniques can be applied to logistic regression too!\n",
        "This follows the same idea -- we add a penalty for model coefficients being too large (this time to the logistic regression loss function).\n",
        "In the questions below, you will use sklearn functions to fit logistic regressions with regularization.\n",
        "\n",
        "You will train **three** logistic regression models with different regularization approaches and compare them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT8YvkrToLzd"
      },
      "source": [
        "#### Background: Regularization in sklearn's LogisticRegression\n",
        "\n",
        "sklearn's `LogisticRegression` uses the parameter `C` (inverse of regularization strength):\n",
        "- **Smaller C** = **stronger regularization** (more penalty, simpler model)\n",
        "- **Larger C** = **weaker regularization** (less penalty, more complex model)\n",
        "\n",
        "The `penalty` parameter controls the type:\n",
        "- `penalty='l1'`: L1 regularization (Lasso) - encourages sparsity\n",
        "- `penalty='l2'`: L2 regularization (Ridge) - shrinks coefficients\n",
        "- `penalty=None`: No regularization\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "model = LogisticRegression(penalty='l2', C=1.0, max_iter=1000,\n",
        "                          random_state=42, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "**Important solver notes**:\n",
        "- Use `solver='liblinear'` for L1 or L2 with small/medium datasets\n",
        "- Use `solver='lbfgs'` for L2 or no penalty\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v378qzNYoLzd"
      },
      "source": [
        "#### Task 5.2.1: Train L1 (Lasso) Model (5 points)\n",
        "\n",
        "Train a logistic regression model with L1 regularization.\n",
        "\n",
        "**Requirements**:\n",
        "- Use `penalty='l1'`\n",
        "- Use `C=0.1` (moderate regularization)\n",
        "- Set `max_iter=1000`, `random_state=42`, `solver='liblinear'`\n",
        "- Name the model `model_l1`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhlOn69VoLzd"
      },
      "outputs": [],
      "source": [
        "# TODO: Train L1 model (LogisticRegression is imported in the first cell)\n",
        "\n",
        "### WRITE YOUR CODE BELOW\n",
        "\n",
        "model_l1 = LogisticRegression(penalty='l1', C=0.1, max_iter=1000, random_state=42, solver='liblinear')\n",
        "model_l1.fit(X_train, y_train)\n",
        "\n",
        "### END CODE HERE\n",
        "\n",
        "print(\"‚úì L1 model trained!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ldIXDNFoLzd"
      },
      "source": [
        "#### Task 5.2.2: Train L2 (Ridge) Model (5 points)\n",
        "\n",
        "Train a logistic regression model with L2 regularization.\n",
        "\n",
        "**Requirements**:\n",
        "- Use `penalty='l2'`\n",
        "- Use `C=1.0` (weak regularization)\n",
        "- Set `max_iter=1000`, `random_state=42`, `solver='liblinear'`\n",
        "- Name the model `model_l2`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIenDF8moLzd"
      },
      "outputs": [],
      "source": [
        "# TODO: Train L2 model\n",
        "\n",
        "### WRITE YOUR CODE BELOW\n",
        "\n",
        "model_l2 = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, random_state=42, solver='liblinear')\n",
        "model_l2.fit(X_train, y_train)\n",
        "\n",
        "### END CODE HERE\n",
        "\n",
        "print(\"‚úì L2 model trained!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbNNRUgwoLzd"
      },
      "source": [
        "#### Task 5.2.3: Train No Regularization Model (5 points)\n",
        "\n",
        "Train a logistic regression model without regularization as a baseline.\n",
        "\n",
        "**Requirements**:\n",
        "- Use `penalty=None`\n",
        "- Set `max_iter=1000`, `random_state=42`, `solver='lbfgs'`\n",
        "- Name the model `model_none`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhiN8SOOoLzd"
      },
      "outputs": [],
      "source": [
        "# TODO: Train model without regularization\n",
        "\n",
        "### WRITE YOUR CODE BELOW\n",
        "\n",
        "model_none = LogisticRegression(penalty=None, max_iter=1000, random_state=42, solver='lbfgs')\n",
        "model_none.fit(X_train, y_train)\n",
        "\n",
        "### END CODE HERE\n",
        "\n",
        "print(\"‚úì Baseline model trained!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr9WzhbUoLze"
      },
      "source": [
        "### 5.3: Evaluate and Compare Models (10 points)\n",
        "\n",
        "Now evaluate all three models on the test set and compare their performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1K1Xk8VoLze"
      },
      "source": [
        "#### Task 5.3.1: Get Predictions for All Models (3 points)\n",
        "\n",
        "Get test set predictions from all three models.\n",
        "\n",
        "**Requirements**:\n",
        "- Use `.predict()` method\n",
        "- Convert to Python lists using `.tolist()`\n",
        "- Name variables: `y_pred_l1`, `y_pred_l2`, `y_pred_none`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2eUw9RAoLze"
      },
      "outputs": [],
      "source": [
        "# TODO: Get predictions from all models\n",
        "\n",
        "### WRITE YOUR CODE BELOW\n",
        "\n",
        "# L1 predictions\n",
        "y_pred_l1 = model_l1.predict(X_test).tolist()\n",
        "\n",
        "# L2 predictions\n",
        "y_pred_l2 = model_l2.predict(X_test).tolist()\n",
        "\n",
        "# No regularization predictions\n",
        "y_pred_none = model_none.predict(X_test).tolist()\n",
        "\n",
        "### END CODE HERE\n",
        "\n",
        "print(\"‚úì Predictions generated!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLYYqE48oLze"
      },
      "source": [
        "#### Task 5.3.2: Compute Metrics for All Models (4 points)\n",
        "\n",
        "Use your `compute_all_binary_metrics` function to evaluate each model.\n",
        "\n",
        "**Requirements**:\n",
        "- Use your implemented function (not sklearn metrics)\n",
        "- Store results in variables: `metrics_l1`, `metrics_l2`, `metrics_none`\n",
        "- Convert y_test to list if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxPmGMQOoLze"
      },
      "outputs": [],
      "source": [
        "# TODO: Compute metrics using your function\n",
        "\n",
        "### WRITE YOUR CODE BELOW\n",
        "\n",
        "# Convert y_test to list\n",
        "y_test_list = y_test.tolist()\n",
        "\n",
        "# Compute metrics for each model\n",
        "metrics_l1 = compute_all_binary_metrics(y_test_list, y_pred_l1)\n",
        "metrics_l2 = compute_all_binary_metrics(y_test_list, y_pred_l2)\n",
        "metrics_none = compute_all_binary_metrics(y_test_list, y_pred_none)\n",
        "\n",
        "### END CODE HERE\n",
        "\n",
        "print(\"‚úì Metrics computed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOj9Bv6EoLze"
      },
      "source": [
        "#### Task 5.3.3: Analyze Feature Usage (3 points)\n",
        "\n",
        "Count how many features each model uses (non-zero coefficients).\n",
        "\n",
        "**Requirements**:\n",
        "- Access coefficients using `.coef_[0]`\n",
        "- Count features with absolute value > 0.0001\n",
        "- Store in: `features_l1`, `features_l2`, `features_none`\n",
        "\n",
        "**Hint**: L1 regularization creates sparsity (many coefficients become exactly zero).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SElri4ICoLze"
      },
      "outputs": [],
      "source": [
        "# TODO: Count non-zero features in each model\n",
        "\n",
        "### WRITE YOUR CODE BELOW\n",
        "\n",
        "features_l1 = sum(1 for c in model_l1.coef_[0] if abs(c) > 0.0001)\n",
        "features_l2 = sum(1 for c in model_l2.coef_[0] if abs(c) > 0.0001)\n",
        "features_none = sum(1 for c in model_none.coef_[0] if abs(c) > 0.0001)\n",
        "\n",
        "### END CODE HERE\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n{'Model':<20} {'Test F1':<12} {'Test Acc':<12} {'Features Used':<15}\")\n",
        "print(\"-\"*60)\n",
        "print(f\"{'L1 (Lasso)':<20} {metrics_l1['f1_score']:<12.4f} {metrics_l1['accuracy']:<12.4f} {features_l1:<15}\")\n",
        "print(f\"{'L2 (Ridge)':<20} {metrics_l2['f1_score']:<12.4f} {metrics_l2['accuracy']:<12.4f} {features_l2:<15}\")\n",
        "print(f\"{'No Regularization':<20} {metrics_none['f1_score']:<12.4f} {metrics_none['accuracy']:<12.4f} {features_none:<15}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3BXinHzoLze"
      },
      "source": [
        "### 5.4: Select and Return Your Best Model (10 points)\n",
        "\n",
        "Based on your comparison, select the best model and return it in the required format for the autograder.\n",
        "\n",
        "**Selection Criteria**: Choose the model with the **highest test F1-score**. If there's a tie, prefer the model with fewer features (more regularization).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAOPw3aVoLze"
      },
      "source": [
        "#### Task 5.4: Create Final Model Output (10 points)\n",
        "\n",
        "Complete the function below to return your best model's results.\n",
        "\n",
        "**Requirements**:\n",
        "1. Determine which model performed best (highest F1)\n",
        "2. Create a dictionary with the exact keys specified\n",
        "3. Use threshold=0.5 for all predictions (already done by `.predict()`)\n",
        "4. Return the dictionary\n",
        "\n",
        "**The autograder will check**:\n",
        "- All required keys present\n",
        "- Correct data types\n",
        "- Predictions are binary (0 or 1)\n",
        "- Predictions have correct length\n",
        "- Model achieves reasonable performance (F1 > 0.50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xB0MaQKUoLze"
      },
      "outputs": [],
      "source": [
        "def select_best_model():\n",
        "    \"\"\"\n",
        "    Select and return the best performing model based on test F1-score.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing:\n",
        "            - 'model_name': str, one of ['l1', 'l2', 'none']\n",
        "            - 'regularization_type': str, one of ['lasso', 'ridge', 'none']\n",
        "            - 'C_value': float, the C parameter used (or None)\n",
        "            - 'test_predictions': list of int, binary predictions on test set\n",
        "            - 'test_metrics': dict, output from compute_all_binary_metrics\n",
        "            - 'num_features_used': int, number of non-zero coefficients\n",
        "            - 'threshold': float, always 0.5 for this assignment\n",
        "    \"\"\"\n",
        "    ### WRITE YOUR CODE BELOW\n",
        "\n",
        "    # Step 1: Determine which model has highest test F1\n",
        "    # Compare metrics_l1['f1_score'], metrics_l2['f1_score'], metrics_none['f1_score']\n",
        "    candidates = [\n",
        "        ('l1', 'lasso', 0.1, y_pred_l1, metrics_l1, features_l1),\n",
        "        ('l2', 'ridge', 1.0, y_pred_l2, metrics_l2, features_l2),\n",
        "        ('none', 'none', None, y_pred_none, metrics_none, features_none)\n",
        "    ]\n",
        "    # Sort by F1 descending, then by fewer features (ascending) for tiebreak\n",
        "    best = max(candidates, key=lambda x: (x[4]['f1_score'], -x[5]))\n",
        "\n",
        "    # Step 2: Based on best model, set the appropriate values\n",
        "    model_name, reg_type, c_val, preds, metrics, n_feat = best\n",
        "\n",
        "    # Step 3: Create the output dictionary\n",
        "    result = {\n",
        "        'model_name': model_name,\n",
        "        'regularization_type': reg_type,\n",
        "        'C_value': c_val,\n",
        "        'test_predictions': preds,\n",
        "        'test_metrics': metrics,\n",
        "        'num_features_used': n_feat,\n",
        "        'threshold': 0.5  # Fixed for this assignment\n",
        "    }\n",
        "\n",
        "    ### END CODE HERE\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMJPGpumoLze"
      },
      "source": [
        "#### Run Your Model Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joBzJEN8oLze"
      },
      "outputs": [],
      "source": [
        "### DO NOT MODIFY THIS CELL ###\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL MODEL SELECTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get your best model\n",
        "final_results = select_best_model()\n",
        "\n",
        "# Display results\n",
        "print(f\"\\nSelected Model: {final_results['model_name'].upper()}\")\n",
        "print(f\"  Regularization: {final_results['regularization_type']}\")\n",
        "print(f\"  C value: {final_results['C_value']}\")\n",
        "print(f\"  Features used: {final_results['num_features_used']} / {len(feature_cols)}\")\n",
        "print(f\"  Threshold: {final_results['threshold']}\")\n",
        "\n",
        "print(f\"\\nTest Set Performance:\")\n",
        "for metric, value in final_results['test_metrics'].items():\n",
        "    print(f\"  {metric}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHU4uRUVoLzf"
      },
      "source": [
        "## Submission\n",
        "\n",
        "Make sure you have:\n",
        "1. Filled in your AI usage disclosure\n",
        "2. Implemented all functions between the marked regions\n",
        "3. Not modified any cells marked with `### DO NOT MODIFY THIS CELL ###`\n",
        "4. Tested your code using the test cells provided\n",
        "\n",
        "**Grading:**\n",
        "- Section 1 (Classification Evaluation Metrics): 15 points\n",
        "- Section 2 (Linear Regression): 15 points\n",
        "- Section 3 (Regularization): 15 points\n",
        "- Section 4 (Logistic Regression): 15 points\n",
        "- Section 6 (Building Your Text Classification Model): 40 points\n",
        "- **Total: 100 points** Good luck!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1UNpfZ7B_Db"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
